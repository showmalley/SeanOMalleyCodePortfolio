{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation / Policy Iteration\n",
    "### Sean O'Malley\n",
    "\n",
    "Reinforcement learning is a subset of machine learning that solves sequential decision making problems, learning from a critic and reward structure. A notion core to reinforcement learning is how an agent behaves and learns from its environment. A key tool for an agent to learn from its environment is that of iterative policy evaluation and policy iteration.\n",
    "\n",
    "__Conceptually__, in the iterative policy evaluation and policy iteration process, the agent has a metric (discrete|continuous) that it values. Given its interactions with an environment, the agent has a policy as to how to best maximize this reward. This policy is valued based on the reward received from the policy at a certain state. This policy can be updated based on the reward of that state, in comparison to the previous state. The moment the previous state policy and current state policy match, an optimal policy is reached. The iterative policy evaluation determines the effectiveness of a policy and policy iteration is the mechanism that updates that policy.\n",
    "\n",
    "__Formally__, policy iteration manipulates the policy directly, rather than finding it indirectly via the optimal value function. In policy iteration, the policy acts greedily towards the best expected result. Eventually the policy would reach a point where continuing to iterate would no longer change anything (convergence).\n",
    "\n",
    "How good is a policy? We know this from the value function.\n",
    "\n",
    "Value iteration only cares about the value function and not on forming a complete policy for every iteration. It is the same policy iteration, but only performing policy evaluation once and then changing the policy right away.\n",
    "\n",
    "While this requires less steps to find the optimal policy, intermediate steps cannot be used as a suboptimal policy as the values do not correspond to a concrete policy, this is where policy iteration is key.\n",
    "\n",
    "The policy iteration process follows the following algorithmic cadence:\n",
    "\n",
    "* choose arbitrary policy \n",
    "    * loop:\n",
    "        * compute the value of the policy\n",
    "        * solve linear equations\n",
    "        * improve the policy at each state\n",
    "    * until new policy equals old policy\n",
    "    \n",
    "At the end of the day, the value function of a policy is just the expected infinite & discounted reward that will be gained, at each state, by executing that policy. \n",
    "\n",
    "Once we know the value of each state under the current policy, we consider whether the value could be improved by changing the first action taken. \n",
    "\n",
    "If so, the policy is altered to take the new action whenever it is in that situation. This step is guaranteed to improve performance of the policy when no improvement is possible, the policy is guaranteed to be optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation\n",
    "\n",
    "The code below shows that policy iteration process via a series of functions to solve a gridworld problem.\n",
    "\n",
    "* given a policy, let's find it's value function V(s)\n",
    "* 2 sources of randomness\n",
    "    * p(a|s) - deciding what action to take given the state\n",
    "    * p(s',r|s,a) - the next state and reward given your action-state pair\n",
    "    * we are only modeling p(a|s) = uniform\n",
    "* __gamma__ - is used to determine the discount factor. A discount factor is useful for teaching an agent to value present values over future values. The example below shows how the discount factor helps the agent not only choose the path that maximizes the reward, but also the path that does so as fast as possible.\n",
    "\n",
    "How would the code change if p(s',r|s,a) is not deterministic?\n",
    "\n",
    "* A non-deterministic policy assumes that taking the same action in the same state on two different occasions may result in different next states and/or different reinforcement values.\n",
    "* If the below problem was non-deterministic, we we would redefine the policy by taking expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   0.5\n",
      "2   0.138888888889\n",
      "3   0.0841049382716\n",
      "4   0.0487825788752\n",
      "5   0.0305968745237\n",
      "6   0.020936465372\n",
      "7   0.0154602885385\n",
      "8   0.0125957267443\n",
      "9   0.0106768890958\n",
      "10   0.00911310419528\n",
      "11   0.00780818790411\n",
      "12   0.00670428088687\n",
      "13   0.00576316028922\n",
      "14   0.00495734073685\n",
      "15   0.00426570920199\n",
      "16   0.00367129309651\n",
      "17   0.00316005081477\n",
      "18   0.00272016465323\n",
      "19   0.00234158959515\n",
      "20   0.00201573935481\n",
      "21   0.00173525141419\n",
      "22   0.00149380152182\n",
      "23   0.00128595197727\n",
      "24   0.00110702482171\n",
      "25   0.000952994480954\n",
      "values for uniformly random actions:\n",
      "---------------------------\n",
      "-0.03| 0.09| 0.22| 0.00|\n",
      "---------------------------\n",
      "-0.16| 0.00|-0.44| 0.00|\n",
      "---------------------------\n",
      "-0.29|-0.41|-0.54|-0.77|\n",
      "\n",
      "\n",
      "\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "1   1.0\n",
      "2   0.99\n",
      "3   0.970299\n",
      "4   0\n",
      "values for fixed policy:\n",
      "---------------------------\n",
      " 0.98| 0.99| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.97| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.96| 0.97| 0.98| 0.97|\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from grid_world import standard_grid\n",
    "\n",
    "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
    "\n",
    "def print_values(V, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      v = V.get((i,j), 0)\n",
    "      if v >= 0:\n",
    "        print(\" %.2f|\" % v, end=\"\")\n",
    "      else:\n",
    "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def print_policy(P, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      a = P.get((i,j), ' ')\n",
    "      print(\"  %s  |\" % a, end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # iterative policy evaluation\n",
    "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # states will be positions (i,j)\n",
    "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
    "  # that can only be at one position at a time\n",
    "  states = grid.all_states()\n",
    "\n",
    "  ### uniformly random actions ###\n",
    "  # initialize V(s) = 0\n",
    "  V = {}\n",
    "  for s in states:\n",
    "    V[s] = 0\n",
    "  gamma = 1.0 # discount factor\n",
    "  # repeat until convergence\n",
    "  count = 0\n",
    "  while True:\n",
    "    count += 1\n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in grid.actions:\n",
    "\n",
    "        new_v = 0 # we will accumulate the answer\n",
    "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
    "        for a in grid.actions[s]:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
    "        V[s] = new_v\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    print(count, \" \", biggest_change)\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for uniformly random actions:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"\\n\\n\")\n",
    "\n",
    "  ### fixed policy ###\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s) = 0\n",
    "  V = {}\n",
    "  for s in states:\n",
    "    V[s] = 0\n",
    "\n",
    "  # let's see how V(s) changes as we get further away from the reward\n",
    "  gamma = 0.99 # discount factor\n",
    "\n",
    "  # repeat until convergence\n",
    "  count = 0\n",
    "  while True:\n",
    "    count += 1\n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        a = policy[s]\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a)\n",
    "        V[s] = r + gamma * V[grid.current_state()]\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    print(count, \" \", biggest_change)\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for fixed policy:\")\n",
    "  print_values(V, grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In iterative policy evaluation we can see the values of each space in our grid given our final policy.\n",
    "\n",
    "### Policy Iteration\n",
    "\n",
    "This deterministic grid gives you a reward of -0.1 for every non-terminal state to encourage finding a shorter path to the goal.\n",
    "\n",
    "The algorithm follows the the subsequent steps:\n",
    "* rewards\n",
    "* state -> action\n",
    "* randomly choose an action and update as we learn\n",
    "* initial policy\n",
    "    * initialize terminal state\n",
    "    * evaluate \n",
    "    * improve\n",
    "* repeat until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "initial policy:\n",
      "---------------------------\n",
      "  U  |  L  |  D  |     |\n",
      "---------------------------\n",
      "  R  |     |  R  |     |\n",
      "---------------------------\n",
      "  D  |  L  |  R  |  U  |\n",
      "s (0, 1) a U grid.current_state() (0, 1) r -0.1 Value -0.9918002192500368\n",
      "s (0, 1) a D grid.current_state() (0, 1) r -0.1 Value -0.9918002192500368\n",
      "s (0, 1) a L grid.current_state() (0, 0) r -0.1 Value -0.9918002192500368\n",
      "s (0, 1) a R grid.current_state() (0, 2) r -0.1 Value -1.0\n",
      "s (1, 2) a U grid.current_state() (0, 2) r -0.1 Value -1.0\n",
      "s (1, 2) a D grid.current_state() (2, 2) r -0.1 Value -1.0\n",
      "s (1, 2) a L grid.current_state() (1, 2) r -0.1 Value -1.0\n",
      "s (1, 2) a R grid.current_state() (1, 3) r -1 Value 0\n",
      "s (0, 0) a U grid.current_state() (0, 0) r -0.1 Value -0.9918002192500368\n",
      "s (0, 0) a D grid.current_state() (1, 0) r -0.1 Value -0.9922986645295516\n",
      "s (0, 0) a L grid.current_state() (0, 0) r -0.1 Value -0.9918002192500368\n",
      "s (0, 0) a R grid.current_state() (0, 1) r -0.1 Value -0.9918002192500368\n",
      "s (2, 1) a U grid.current_state() (2, 1) r -0.1 Value -0.9944266885052014\n",
      "s (2, 1) a D grid.current_state() (2, 1) r -0.1 Value -0.9944266885052014\n",
      "s (2, 1) a L grid.current_state() (2, 0) r -0.1 Value -0.9944266885052014\n",
      "s (2, 1) a R grid.current_state() (2, 2) r -0.1 Value -1.0\n",
      "s (2, 0) a U grid.current_state() (1, 0) r -0.1 Value -0.9922986645295516\n",
      "s (2, 0) a D grid.current_state() (2, 0) r -0.1 Value -0.9944266885052014\n",
      "s (2, 0) a L grid.current_state() (2, 0) r -0.1 Value -0.9944266885052014\n",
      "s (2, 0) a R grid.current_state() (2, 1) r -0.1 Value -0.9944266885052014\n",
      "s (2, 3) a U grid.current_state() (1, 3) r -1 Value 0\n",
      "s (2, 3) a D grid.current_state() (2, 3) r -0.1 Value -1.0\n",
      "s (2, 3) a L grid.current_state() (2, 2) r -0.1 Value -1.0\n",
      "s (2, 3) a R grid.current_state() (2, 3) r -0.1 Value -1.0\n",
      "s (2, 2) a U grid.current_state() (1, 2) r -0.1 Value -1.0\n",
      "s (2, 2) a D grid.current_state() (2, 2) r -0.1 Value -1.0\n",
      "s (2, 2) a L grid.current_state() (2, 1) r -0.1 Value -0.9944266885052014\n",
      "s (2, 2) a R grid.current_state() (2, 3) r -0.1 Value -1.0\n",
      "s (1, 0) a U grid.current_state() (0, 0) r -0.1 Value -0.9918002192500368\n",
      "s (1, 0) a D grid.current_state() (2, 0) r -0.1 Value -0.9944266885052014\n",
      "s (1, 0) a L grid.current_state() (1, 0) r -0.1 Value -0.9922986645295516\n",
      "s (1, 0) a R grid.current_state() (1, 0) r -0.1 Value -0.9922986645295516\n",
      "s (0, 2) a U grid.current_state() (0, 2) r -0.1 Value -1.0\n",
      "s (0, 2) a D grid.current_state() (1, 2) r -0.1 Value -1.0\n",
      "s (0, 2) a L grid.current_state() (0, 1) r -0.1 Value -0.9918002192500368\n",
      "s (0, 2) a R grid.current_state() (0, 3) r 1 Value 0\n",
      "1\n",
      "s (0, 1) a U grid.current_state() (0, 1) r -0.1 Value -0.9940223598332769\n",
      "s (0, 1) a D grid.current_state() (0, 1) r -0.1 Value -0.9940223598332769\n",
      "s (0, 1) a L grid.current_state() (0, 0) r -0.1 Value -0.9940223598332769\n",
      "s (0, 1) a R grid.current_state() (0, 2) r -0.1 Value 1.0\n",
      "s (1, 2) a U grid.current_state() (0, 2) r -0.1 Value 1.0\n",
      "s (1, 2) a D grid.current_state() (2, 2) r -0.1 Value -0.9963433503282626\n",
      "s (1, 2) a L grid.current_state() (1, 2) r -0.1 Value 0.8\n",
      "s (1, 2) a R grid.current_state() (1, 3) r -1 Value 0\n",
      "s (0, 0) a U grid.current_state() (0, 0) r -0.1 Value -0.9940223598332769\n",
      "s (0, 0) a D grid.current_state() (1, 0) r -0.1 Value -0.9946201238499492\n",
      "s (0, 0) a L grid.current_state() (0, 0) r -0.1 Value -0.9940223598332769\n",
      "s (0, 0) a R grid.current_state() (0, 1) r -0.1 Value -0.9940223598332769\n",
      "s (2, 1) a U grid.current_state() (2, 1) r -0.1 Value -0.9959370559202918\n",
      "s (2, 1) a D grid.current_state() (2, 1) r -0.1 Value -0.9959370559202918\n",
      "s (2, 1) a L grid.current_state() (2, 0) r -0.1 Value -0.9946201238499492\n",
      "s (2, 1) a R grid.current_state() (2, 2) r -0.1 Value -0.9963433503282626\n",
      "s (2, 0) a U grid.current_state() (1, 0) r -0.1 Value -0.9946201238499492\n",
      "s (2, 0) a D grid.current_state() (2, 0) r -0.1 Value -0.9946201238499492\n",
      "s (2, 0) a L grid.current_state() (2, 0) r -0.1 Value -0.9946201238499492\n",
      "s (2, 0) a R grid.current_state() (2, 1) r -0.1 Value -0.9959370559202918\n",
      "s (2, 3) a U grid.current_state() (1, 3) r -1 Value 0\n",
      "s (2, 3) a D grid.current_state() (2, 3) r -0.1 Value -1.0\n",
      "s (2, 3) a L grid.current_state() (2, 2) r -0.1 Value -0.9963433503282626\n",
      "s (2, 3) a R grid.current_state() (2, 3) r -0.1 Value -1.0\n",
      "s (2, 2) a U grid.current_state() (1, 2) r -0.1 Value 0.8\n",
      "s (2, 2) a D grid.current_state() (2, 2) r -0.1 Value -0.9963433503282626\n",
      "s (2, 2) a L grid.current_state() (2, 1) r -0.1 Value -0.9959370559202918\n",
      "s (2, 2) a R grid.current_state() (2, 3) r -0.1 Value -1.0\n",
      "s (1, 0) a U grid.current_state() (0, 0) r -0.1 Value -0.9940223598332769\n",
      "s (1, 0) a D grid.current_state() (2, 0) r -0.1 Value -0.9946201238499492\n",
      "s (1, 0) a L grid.current_state() (1, 0) r -0.1 Value -0.9946201238499492\n",
      "s (1, 0) a R grid.current_state() (1, 0) r -0.1 Value -0.9946201238499492\n",
      "s (0, 2) a U grid.current_state() (0, 2) r -0.1 Value 1.0\n",
      "s (0, 2) a D grid.current_state() (1, 2) r -0.1 Value 0.8\n",
      "s (0, 2) a L grid.current_state() (0, 1) r -0.1 Value -0.9940223598332769\n",
      "s (0, 2) a R grid.current_state() (0, 3) r 1 Value 0\n",
      "2\n",
      "s (0, 1) a U grid.current_state() (0, 1) r -0.1 Value 0.8\n",
      "s (0, 1) a D grid.current_state() (0, 1) r -0.1 Value 0.8\n",
      "s (0, 1) a L grid.current_state() (0, 0) r -0.1 Value -0.9956423003184589\n",
      "s (0, 1) a R grid.current_state() (0, 2) r -0.1 Value 1.0\n",
      "s (1, 2) a U grid.current_state() (0, 2) r -0.1 Value 1.0\n",
      "s (1, 2) a D grid.current_state() (2, 2) r -0.1 Value 0.6200000000000001\n",
      "s (1, 2) a L grid.current_state() (1, 2) r -0.1 Value 0.8\n",
      "s (1, 2) a R grid.current_state() (1, 3) r -1 Value 0\n",
      "s (0, 0) a U grid.current_state() (0, 0) r -0.1 Value -0.9956423003184589\n",
      "s (0, 0) a D grid.current_state() (1, 0) r -0.1 Value -0.996078070286613\n",
      "s (0, 0) a L grid.current_state() (0, 0) r -0.1 Value -0.9956423003184589\n",
      "s (0, 0) a R grid.current_state() (0, 1) r -0.1 Value 0.8\n",
      "s (2, 1) a U grid.current_state() (2, 1) r -0.1 Value -0.996078070286613\n",
      "s (2, 1) a D grid.current_state() (2, 1) r -0.1 Value -0.996078070286613\n",
      "s (2, 1) a L grid.current_state() (2, 0) r -0.1 Value -0.996078070286613\n",
      "s (2, 1) a R grid.current_state() (2, 2) r -0.1 Value 0.6200000000000001\n",
      "s (2, 0) a U grid.current_state() (1, 0) r -0.1 Value -0.996078070286613\n",
      "s (2, 0) a D grid.current_state() (2, 0) r -0.1 Value -0.996078070286613\n",
      "s (2, 0) a L grid.current_state() (2, 0) r -0.1 Value -0.996078070286613\n",
      "s (2, 0) a R grid.current_state() (2, 1) r -0.1 Value -0.996078070286613\n",
      "s (2, 3) a U grid.current_state() (1, 3) r -1 Value 0\n",
      "s (2, 3) a D grid.current_state() (2, 3) r -0.1 Value 0.4580000000000002\n",
      "s (2, 3) a L grid.current_state() (2, 2) r -0.1 Value 0.6200000000000001\n",
      "s (2, 3) a R grid.current_state() (2, 3) r -0.1 Value 0.4580000000000002\n",
      "s (2, 2) a U grid.current_state() (1, 2) r -0.1 Value 0.8\n",
      "s (2, 2) a D grid.current_state() (2, 2) r -0.1 Value 0.6200000000000001\n",
      "s (2, 2) a L grid.current_state() (2, 1) r -0.1 Value -0.996078070286613\n",
      "s (2, 2) a R grid.current_state() (2, 3) r -0.1 Value 0.4580000000000002\n",
      "s (1, 0) a U grid.current_state() (0, 0) r -0.1 Value -0.9956423003184589\n",
      "s (1, 0) a D grid.current_state() (2, 0) r -0.1 Value -0.996078070286613\n",
      "s (1, 0) a L grid.current_state() (1, 0) r -0.1 Value -0.996078070286613\n",
      "s (1, 0) a R grid.current_state() (1, 0) r -0.1 Value -0.996078070286613\n",
      "s (0, 2) a U grid.current_state() (0, 2) r -0.1 Value 1.0\n",
      "s (0, 2) a D grid.current_state() (1, 2) r -0.1 Value 0.8\n",
      "s (0, 2) a L grid.current_state() (0, 1) r -0.1 Value 0.8\n",
      "s (0, 2) a R grid.current_state() (0, 3) r 1 Value 0\n",
      "3\n",
      "s (0, 1) a U grid.current_state() (0, 1) r -0.1 Value 0.8\n",
      "s (0, 1) a D grid.current_state() (0, 1) r -0.1 Value 0.8\n",
      "s (0, 1) a L grid.current_state() (0, 0) r -0.1 Value 0.6200000000000001\n",
      "s (0, 1) a R grid.current_state() (0, 2) r -0.1 Value 1.0\n",
      "s (1, 2) a U grid.current_state() (0, 2) r -0.1 Value 1.0\n",
      "s (1, 2) a D grid.current_state() (2, 2) r -0.1 Value 0.6200000000000001\n",
      "s (1, 2) a L grid.current_state() (1, 2) r -0.1 Value 0.8\n",
      "s (1, 2) a R grid.current_state() (1, 3) r -1 Value 0\n",
      "s (0, 0) a U grid.current_state() (0, 0) r -0.1 Value 0.6200000000000001\n",
      "s (0, 0) a D grid.current_state() (1, 0) r -0.1 Value 0.4580000000000002\n",
      "s (0, 0) a L grid.current_state() (0, 0) r -0.1 Value 0.6200000000000001\n",
      "s (0, 0) a R grid.current_state() (0, 1) r -0.1 Value 0.8\n",
      "s (2, 1) a U grid.current_state() (2, 1) r -0.1 Value 0.4580000000000002\n",
      "s (2, 1) a D grid.current_state() (2, 1) r -0.1 Value 0.4580000000000002\n",
      "s (2, 1) a L grid.current_state() (2, 0) r -0.1 Value 0.31220000000000014\n",
      "s (2, 1) a R grid.current_state() (2, 2) r -0.1 Value 0.6200000000000001\n",
      "s (2, 0) a U grid.current_state() (1, 0) r -0.1 Value 0.4580000000000002\n",
      "s (2, 0) a D grid.current_state() (2, 0) r -0.1 Value 0.31220000000000014\n",
      "s (2, 0) a L grid.current_state() (2, 0) r -0.1 Value 0.31220000000000014\n",
      "s (2, 0) a R grid.current_state() (2, 1) r -0.1 Value 0.4580000000000002\n",
      "s (2, 3) a U grid.current_state() (1, 3) r -1 Value 0\n",
      "s (2, 3) a D grid.current_state() (2, 3) r -0.1 Value 0.4580000000000002\n",
      "s (2, 3) a L grid.current_state() (2, 2) r -0.1 Value 0.6200000000000001\n",
      "s (2, 3) a R grid.current_state() (2, 3) r -0.1 Value 0.4580000000000002\n",
      "s (2, 2) a U grid.current_state() (1, 2) r -0.1 Value 0.8\n",
      "s (2, 2) a D grid.current_state() (2, 2) r -0.1 Value 0.6200000000000001\n",
      "s (2, 2) a L grid.current_state() (2, 1) r -0.1 Value 0.4580000000000002\n",
      "s (2, 2) a R grid.current_state() (2, 3) r -0.1 Value 0.4580000000000002\n",
      "s (1, 0) a U grid.current_state() (0, 0) r -0.1 Value 0.6200000000000001\n",
      "s (1, 0) a D grid.current_state() (2, 0) r -0.1 Value 0.31220000000000014\n",
      "s (1, 0) a L grid.current_state() (1, 0) r -0.1 Value 0.4580000000000002\n",
      "s (1, 0) a R grid.current_state() (1, 0) r -0.1 Value 0.4580000000000002\n",
      "s (0, 2) a U grid.current_state() (0, 2) r -0.1 Value 1.0\n",
      "s (0, 2) a D grid.current_state() (1, 2) r -0.1 Value 0.8\n",
      "s (0, 2) a L grid.current_state() (0, 1) r -0.1 Value 0.8\n",
      "s (0, 2) a R grid.current_state() (0, 3) r 1 Value 0\n",
      "4\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# this is deterministic\n",
    "# all p(s',r|s,a) = 1 or 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "  # we want to see if this will encourage finding a shorter path to the goal\n",
    "  grid = negative_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # we'll randomly choose an action and update as we learn\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initial policy\n",
    "  print(\"initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s)\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    # V[s] = 0\n",
    "    if s in grid.actions:\n",
    "      V[s] = np.random.random()\n",
    "    else:\n",
    "      # terminal state\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence - will break out when policy does not change\n",
    "  count = 0\n",
    "  while True:\n",
    "     \n",
    "    count += 1\n",
    "    # policy evaluation step - we already know how to do this!\n",
    "    while True:\n",
    "      \n",
    "      biggest_change = 0\n",
    "      for s in states:\n",
    "        old_v = V[s]\n",
    "\n",
    "        # V(s) only has value if it's not a terminal state\n",
    "        if s in policy:\n",
    "          a = policy[s]\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          V[s] = r + GAMMA * V[grid.current_state()]\n",
    "          biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "      if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "\n",
    "    # policy improvement step\n",
    "    is_policy_converged = True\n",
    "    for s in states:\n",
    "      if s in policy:\n",
    "        old_a = policy[s]\n",
    "        new_a = None\n",
    "        best_value = float('-inf')\n",
    "        # loop through all possible actions to find the best current action\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          print(\"s\", s, \"a\", a, \"grid.current_state()\", grid.current_state(), \"r\", r, \"Value\", V[grid.current_state()])\n",
    "          if v > best_value:\n",
    "            best_value = v\n",
    "            new_a = a\n",
    "        policy[s] = new_a\n",
    "        if new_a != old_a:\n",
    "          is_policy_converged = False\n",
    "\n",
    "    print(count)    \n",
    "    if is_policy_converged:\n",
    "      break\n",
    "\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
