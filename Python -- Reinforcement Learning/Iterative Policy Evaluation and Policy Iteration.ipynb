{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation / Policy Iteration\n",
    "### Sean O'Malley\n",
    "\n",
    "\n",
    "Policy Iteration manipulates the policy directly, rather than finding it indirectly via the optimal value function. \n",
    "\n",
    "In policy iteration, the policy acts greedily towards the best expected result. Eventually the policy would reach a point where continuing to iterate would no longer change anything (convergence).\n",
    "\n",
    "How good is a policy? We know this from the value function.\n",
    "\n",
    "Value iteration only cares about the value function and not on forming a complete policy for every iteration. It is the same policy iteration, but only performing policy evaluation once and then changing the policy right away.\n",
    "\n",
    "While this requires less steps to find the optimal policy, intermediate steps cannot be used as a suboptimal policy as the values do not correspond to a concrete policy, this is where policy iteration is key.\n",
    "\n",
    "The policy iteration process follows the following algorithmic cadence:\n",
    "\n",
    "* choose arbitrary policy \n",
    "    * loop:\n",
    "        * compute the value of the policy\n",
    "        * solve linear equations\n",
    "        * improve the policy at each state\n",
    "    * until new policy equals old policy\n",
    "    \n",
    "At the end of the day, the value function of a policy is just the expected infinite & discounted reward that will be gained, at each state, by executing that policy. \n",
    "\n",
    "Once we know the value of each state under the current policy, we consider whether the value could be improved by changing the first action taken. \n",
    "\n",
    "If so, the policy is altered to take the new action whenever it is in that situation. This step is guaranteed to improve performance of the policy when no improvement is possible, the policy is guaranteed to be optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   0.5\n",
      "2   0.1388888888888889\n",
      "3   0.08410493827160492\n",
      "4   0.048782578875171456\n",
      "5   0.030596874523700635\n",
      "6   0.020936465371979185\n",
      "7   0.015460288538467815\n",
      "8   0.012595726744316582\n",
      "9   0.010676889095810493\n",
      "10   0.009113104195280997\n",
      "11   0.00780818790410881\n",
      "12   0.006704280886868336\n",
      "13   0.0057631602892224965\n",
      "14   0.004957340736853388\n",
      "15   0.0042657092019864895\n",
      "16   0.0036712930965119295\n",
      "17   0.0031600508147732187\n",
      "18   0.002720164653231494\n",
      "19   0.0023415895951464094\n",
      "20   0.0020157393548111413\n",
      "21   0.0017352514141919517\n",
      "22   0.0014938015218150524\n",
      "23   0.001285951977272659\n",
      "24   0.001107024821710545\n",
      "25   0.0009529944809544277\n",
      "values for uniformly random actions:\n",
      "---------------------------\n",
      "-0.03| 0.09| 0.22| 0.00|\n",
      "---------------------------\n",
      "-0.16| 0.00|-0.44| 0.00|\n",
      "---------------------------\n",
      "-0.29|-0.41|-0.54|-0.77|\n",
      "\n",
      "\n",
      "\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "1   1.0\n",
      "2   0.99\n",
      "3   0.9702989999999999\n",
      "4   0\n",
      "values for fixed policy:\n",
      "---------------------------\n",
      " 0.98| 0.99| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.97| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.96| 0.97| 0.98| 0.97|\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "from grid_world import standard_grid\n",
    "\n",
    "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
    "\n",
    "def print_values(V, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      v = V.get((i,j), 0)\n",
    "      if v >= 0:\n",
    "        print(\" %.2f|\" % v, end=\"\")\n",
    "      else:\n",
    "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def print_policy(P, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      a = P.get((i,j), ' ')\n",
    "      print(\"  %s  |\" % a, end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # iterative policy evaluation\n",
    "  # given a policy, let's find it's value function V(s)\n",
    "  # we will do this for both a uniform random policy and fixed policy\n",
    "  # NOTE:\n",
    "  # there are 2 sources of randomness\n",
    "  # p(a|s) - deciding what action to take given the state\n",
    "  # p(s',r|s,a) - the next state and reward given your action-state pair\n",
    "  # we are only modeling p(a|s) = uniform\n",
    "  # how would the code change if p(s',r|s,a) is not deterministic?\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # states will be positions (i,j)\n",
    "  # simpler than tic-tac-toe because we only have one \"game piece\"\n",
    "  # that can only be at one position at a time\n",
    "  states = grid.all_states()\n",
    "\n",
    "  ### uniformly random actions ###\n",
    "  # initialize V(s) = 0\n",
    "  V = {}\n",
    "  for s in states:\n",
    "    V[s] = 0\n",
    "  gamma = 1.0 # discount factor\n",
    "  # repeat until convergence\n",
    "  count = 0\n",
    "  while True:\n",
    "    count += 1\n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in grid.actions:\n",
    "\n",
    "        new_v = 0 # we will accumulate the answer\n",
    "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
    "        for a in grid.actions[s]:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
    "        V[s] = new_v\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    print(count, \" \", biggest_change)\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for uniformly random actions:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"\\n\\n\")\n",
    "\n",
    "  ### fixed policy ###\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s) = 0\n",
    "  V = {}\n",
    "  for s in states:\n",
    "    V[s] = 0\n",
    "\n",
    "  # let's see how V(s) changes as we get further away from the reward\n",
    "  gamma = 0.99 # discount factor\n",
    "\n",
    "  # repeat until convergence\n",
    "  count = 0\n",
    "  while True:\n",
    "    count += 1\n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        a = policy[s]\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a)\n",
    "        V[s] = r + gamma * V[grid.current_state()]\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    print(count, \" \", biggest_change)\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for fixed policy:\")\n",
    "  print_values(V, grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "initial policy:\n",
      "---------------------------\n",
      "  D  |  L  |  L  |     |\n",
      "---------------------------\n",
      "  L  |     |  L  |     |\n",
      "---------------------------\n",
      "  D  |  U  |  U  |  U  |\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# this is deterministic\n",
    "# all p(s',r|s,a) = 1 or 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "  # we want to see if this will encourage finding a shorter path to the goal\n",
    "  grid = negative_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # we'll randomly choose an action and update as we learn\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initial policy\n",
    "  print(\"initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s)\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    # V[s] = 0\n",
    "    if s in grid.actions:\n",
    "      V[s] = np.random.random()\n",
    "    else:\n",
    "      # terminal state\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence - will break out when policy does not change\n",
    "  while True:\n",
    "\n",
    "    # policy evaluation step - we already know how to do this!\n",
    "    while True:\n",
    "      biggest_change = 0\n",
    "      for s in states:\n",
    "        old_v = V[s]\n",
    "\n",
    "        # V(s) only has value if it's not a terminal state\n",
    "        if s in policy:\n",
    "          a = policy[s]\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          V[s] = r + GAMMA * V[grid.current_state()]\n",
    "          biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "      if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "\n",
    "    # policy improvement step\n",
    "    is_policy_converged = True\n",
    "    for s in states:\n",
    "      if s in policy:\n",
    "        old_a = policy[s]\n",
    "        new_a = None\n",
    "        best_value = float('-inf')\n",
    "        # loop through all possible actions to find the best current action\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          if v > best_value:\n",
    "            best_value = v\n",
    "            new_a = a\n",
    "        policy[s] = new_a\n",
    "        if new_a != old_a:\n",
    "          is_policy_converged = False\n",
    "\n",
    "    if is_policy_converged:\n",
    "      break\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
