{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods for Reinforcement Learning\n",
    "### Sean O'Malley\n",
    "\n",
    "__Topics:__\n",
    "* Policy Update, Evaluation and Iteration\n",
    "* Without Exploring Starts\n",
    "\n",
    "__Policy Update, Evaluation and Iteration:__\n",
    "\n",
    "In reinforcement learning, Monte Carlo prediction is a method used for learning the state-value function for a given policy. Now, remember that the value of a state is the expected return (expected cumulative future discounted reward) starting from a state. So, Monte Carlo methods takes the next logical step to estimate the value of a state, aveage the returns observed after visits to that state. The more states are observed, the average will naturally converge to the expected value. This notion is core to Monte Carlo methodologies.\n",
    "\n",
    "Monte Carlo methods reframe our previous thoughts about determining the value of a reinforcement learning policy at a certain state. \n",
    "\n",
    "Monte Carlo vs. Temporal Difference:\n",
    "\n",
    "* Monte Carlo\n",
    "        * averages Q over sampled paths\n",
    "        * needs full trajectory to learn\n",
    "        * less reliant on Markov property\n",
    "* Temporal difference\n",
    "        * uses recurrent formula for q\n",
    "        * learns from partial trajectory and works with infinite MDP\n",
    "        * needs less experience to learn\n",
    "\n",
    "Unlike policy and value itereation, where policy adjustments are mede across iterations via Markov Decision Processes and the bellman equation, Monte Carlo methods determine the value of a state, under a policy and passing through that state. Each occurrance, or episode, of a state is therefore called a visit. Implying that if a state can visit once, it could surely visit twice, in fact, Monte Carlo methods accuracy relies on states being visited a multitude of times before arriving at its true value. \n",
    "\n",
    "There are two different kinds of Monte Carlo methods to determine the value of a state given a policy $ V_\\pi(s)$:\n",
    "1. First Visit Monte Carlo\n",
    "    * estimates of $V_\\pi(s)$ are the average of returns following first visits to a state\n",
    "2. Every Visit Monte Carlo\n",
    "    * averages the returns following all visits to a state\n",
    "    \n",
    "Algorithmically, every-visit Monte Carlo predition follows the below cadence:\n",
    "\n",
    "* estimate value for a policy Vπ(s)\n",
    "\t* require: initialize\n",
    "\t\t* policy\n",
    "\t\t* arbitrary state value function\n",
    "\t\t* empty list for all states\n",
    "\t\t* repeat\n",
    "\t\t\t* generate episode using π\n",
    "\t\t\t* for all s appearing in the episode:\n",
    "\t\t\t\t* return first occurrence of s\n",
    "\t\t\t\t* append R to Returns(s)\n",
    "\t\t\t\t* V(s) = avg(Returns(s))\n",
    "\t\t\t\t* end\n",
    "\t\t\t* until forever\n",
    "\n",
    "An effective Monte Carlo algorithm needs to ensure that all state action pairs are sampled repeatedly to compute the Q and be confident in that figure. This again is due to the key concept of Monte Carlo episodes allowing the average value of states to converge to their optimal value\n",
    "\n",
    "\n",
    "__Monte Carlo Policy Iteration:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "\n",
    "  # reset game to start at a random position\n",
    "  # we need to do this, because given our current deterministic policy\n",
    "  # we would never end up at certain states, but we still want to measure their value\n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "\n",
    "  s = grid.current_state()\n",
    "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "  while not grid.game_over():\n",
    "    a = policy[s]\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards.append((s, r))\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_and_returns = []\n",
    "  first = True\n",
    "  for s, r in reversed(states_and_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_and_returns.append((s, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_and_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_and_returns\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "\n",
    "  # initialize V(s) and returns\n",
    "  V = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions:\n",
    "      returns[s] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat\n",
    "  for t in range(100):\n",
    "\n",
    "    # generate an episode using pi\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, G in states_and_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      if s not in seen_states:\n",
    "        returns[s].append(G)\n",
    "        V[s] = np.mean(returns[s])\n",
    "        seen_states.add(s)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo optimization behaves much differently that that of the policy and value iteration, most notably, monte carlo methods have a much more polemic view of states of values. We can see that there is really only one direction that the algorithm belives you should go, and all other directions are not only lower in value, but a negative value. \n",
    "\n",
    "__Monte Carlo Methods for Optimization without a Model:__\n",
    "\n",
    "The above policy iteration via Monte Carlo methods is incredibly useful when we have a model available, however sometimes a model is not available and we need more than just the state to develop a policy, we need both the state and action. We need to specifically estimate the value of each action in order for the values to be useful in suggesting a policy and find the optimal Q.\n",
    "\n",
    "Do do so, we must first consider the policy evaluation problem for action values, which is to estimate expected returnwhen starting at a specific state, taking a specific action, and following that policy thereafter $ q_\\pi(s,a) $.\n",
    "\n",
    "Monte Carlo performs the same as with policy iteration, but instead of an episode being the visitng a state, it is visiting a state-action pair. A complication is thus created where some state-action pairs may not be visited and for monte carlo to be effective, those state action pairs need to be visted often. \n",
    "\n",
    "This is a problem we run into often in reinforcement learning, how do we maintain exploration? A primary way to combat this in Monte Carlo policy iteration methods is through through __exploring starts__. Exploring starts is when one specifies that the episodes start in a state-action pair and that every pair begins with a non-zero probability of being selected to start...thus guaranteeing that all state action pairs will be visted an infinate number of times (in the limit of the infinate number of episodes).\n",
    "\n",
    "These methods are used in Monte Carlo estimation to approximate optimal policies through generalized policy iteration, however are not always the most effective route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Monte Carlo Without Exploring Starts:__\n",
    "\n",
    "Exploring starts is an unlikely assumption to make in control Monte Carlo methods for policy iteration and optimal state-value pair policy. So, one thinks, the only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them.\n",
    "\n",
    "there are two approaches to ensuring this:\n",
    "* on-policy methods -- attempt to evaluate or improve the policy that is used to make decisions \n",
    "* off-policy methods -- evaluate or improve a policy different from that used to generate the data\n",
    "\n",
    "On-policy control methods the policy is generally soft, meaning that the policy of an action given a state is greater than 0 for all states and actions as they shift closer to the optimal policy.\n",
    "\n",
    "The below algorithm uses the epsilon soft method, which is more epsilon greedy as it approaches an optimal policy. What this means is that all non-greedy actions are given the minimal probability selection and the remaining bulk of the probability is given the greedy action. The essential idea of on policy Monte Carlo control is still that of generalized policy iteration. \n",
    "\n",
    "We still use first-visit Monte Carlo methods to estimate the action-value function, however, without the assumption of exploring starts. Thus, we can’t improve the policy by making it greedy with respect to the current value function, instead we will move it only towards a greedy policy. \n",
    "\n",
    "For any epsilon soft policy, any greedy policy with respect to the action-value function is guaranteed to be better than or equal to the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final values:\n",
      "---------------------------\n",
      " 0.58| 0.77| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.40| 0.00| 0.76| 0.00|\n",
      "---------------------------\n",
      " 0.25| 0.11|-0.09| 0.00|\n",
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  L  |  D  |\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: find optimal policy and value function\n",
    "#       using on-policy first-visit MC\n",
    "\n",
    "def random_action(a, eps=0.1):\n",
    "  # choose given a with probability 1 - eps + eps/4\n",
    "  # choose some other a' != a with probability eps/4\n",
    "  p = np.random.random()\n",
    "  # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
    "  #   return a\n",
    "  # else:\n",
    "  #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
    "  #   tmp.remove(a)\n",
    "  #   return np.random.choice(tmp)\n",
    "  #\n",
    "  # this is equivalent to the above\n",
    "  if p < (1 - eps):\n",
    "    return a\n",
    "  else:\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "  # in this version we will NOT use \"exploring starts\" method\n",
    "  # instead we will explore using an epsilon-soft policy\n",
    "  s = (2, 0)\n",
    "  grid.set_state(s)\n",
    "  a = random_action(policy[s])\n",
    "\n",
    "  # be aware of the timing\n",
    "  # each triple is s(t), a(t), r(t)\n",
    "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  while True:\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    if grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else:\n",
    "      a = random_action(policy[s]) # the next state is stochastic\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  # grid = standard_grid()\n",
    "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
    "  # in order to minimize number of steps\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # initialize a random policy\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initialize Q(s,a) and returns\n",
    "  Q = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "      Q[s] = {}\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      pass\n",
    "\n",
    "  # repeat until convergence\n",
    "  deltas = []\n",
    "  for t in range(5000):\n",
    "    if t % 1000 == 0:\n",
    "      print(t)\n",
    "\n",
    "    # generate an episode using pi\n",
    "    biggest_change = 0\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      sa = (s, a)\n",
    "      if sa not in seen_state_action_pairs:\n",
    "        old_q = Q[s][a]\n",
    "        returns[sa].append(G)\n",
    "        Q[s][a] = np.mean(returns[sa])\n",
    "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "        seen_state_action_pairs.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "    for s in policy.keys():\n",
    "      a, _ = max_dict(Q[s])\n",
    "      policy[s] = a\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # find the optimal state-value function\n",
    "  # V(s) = max[a]{ Q(s,a) }\n",
    "  V = {}\n",
    "  for s in policy.keys():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "  print(\"final values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"final policy:\")\n",
    "  print_policy(policy, grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we arrive to similar conclusions to that of the traditional Monte Carlo methods where we do have access to a model, decisively determining the optimal policy to maximize reward in the gridworld problem.\n",
    "\n",
    "__Resources:__\n",
    "* lazyprogrammer\n",
    "* Reinforcement Learnging: An Introduction (Sutton)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
