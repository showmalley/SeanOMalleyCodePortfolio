{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "### Sean O'Malley\n",
    "\n",
    "As a reminder, reinforcement learning is an agent interacting with an environment, at each step the agent has an action that leads to the alteration of the environment, thus receiving either a penalty or reward based on that action. The primary goal is to maximize total value of rewards.\n",
    "\n",
    "Markov Decision Processes is a formalized process used to describe the agent's interaction with its environment. An MDP has 5 elements:\n",
    "1. set of states\n",
    "2. set of actions\n",
    "3. state transition model (how env changes due to agent action)\n",
    "4. reward model (real value reward for action)\n",
    "5. discount actor (determines importance of future rewards)\n",
    "\n",
    "Both Value iteration and policy iteration can be to solve Markov Decision Processes as 'offline' update options towards an environment, but in this document we will cover value iteration. \n",
    "\n",
    "__Value Function:__ The core principle to all optmization problems, but especially so with value iteration is that of the value function. The value function represents the quality of a state for an agent and is equal to the total reward for an agent starting at that specific state (total reward is the sum of rewards at each state in consideration to the discount factor). The optimal value function has the highest value\n",
    "\n",
    "__Q-Function:__ In value iteration, the Q-function is a key component to the learning of the algorithm. The Q-function is the state-action pair that returns a real value. The optimal Q-function is the expected total reward an agent receives in a specific state given a specific action. Essentially meaning that the optimal q-function is an indication for the value of an agent's action given a certain state\n",
    "\n",
    "Therefore, if we know the optimal q-funciton, the optimal policy can be easily extracted by choosing the action that gives the maximum q function for a specific state.\n",
    "\n",
    "__Bellman Equation:__ is the recursive definition of the q-function. The q-function equals the sum of the immediate reward after an action in a specific state and the discount of future rewards after the transition to a new state. Value iteration relies heavily upon the bellman equation and finds the optimal policy by the convergence of the value and q function.\n",
    "\n",
    "__Value Iteration__ is the idea that if we know the value of each state, our decision would be to always choose the action that maximizes that value. Value iteration computes the optimal state value function by iteratively improving the value function. \n",
    "\n",
    "First by initializing it with random values, value iteration then repeatedcly updates the q-function and the value function until they converge, guaranteeing the convergence to optimal values.\n",
    "\n",
    "Algorithmically this looks like:\n",
    "\n",
    "* assign each state a random value\n",
    "    * calculate new value baaed on its proximal utilities for each state\n",
    "    * update each state's value based on bellman equation\n",
    "    * if no change, stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Grid World\n",
    "\n",
    "Below we see that we are solving the gridworld problem by determining the optimal steps using value iteration. We can see that we discount future values using a gamma of 0.9, allow ourselves the possible actions of up, down, right and left. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "initial policy:\n",
      "---------------------------\n",
      "  L  |  D  |  L  |     |\n",
      "---------------------------\n",
      "  D  |     |  U  |     |\n",
      "---------------------------\n",
      "  D  |  U  |  D  |  D  |\n",
      "1 0.5120438981916062\n",
      "2 0.3099423529341516\n",
      "3 0.1581921271581198\n",
      "4 0\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# this is deterministic\n",
    "# all p(s',r|s,a) = 1 or 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "  # we want to see if this will encourage finding a shorter path to the goal\n",
    "  grid = negative_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # we'll randomly choose an action and update as we learn\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initial policy\n",
    "  print(\"initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s)\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    # V[s] = 0\n",
    "    if s in grid.actions:\n",
    "      V[s] = np.random.random()\n",
    "    else:\n",
    "      # terminal state\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence\n",
    "  # V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
    "  count = 0\n",
    "  while True:\n",
    "    count += 1\n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        new_v = float('-inf')\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          if v > new_v:\n",
    "            new_v = v\n",
    "        V[s] = new_v\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "        \n",
    "    print(count,biggest_change)\n",
    "\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "\n",
    "  # find a policy that leads to optimal value function\n",
    "  for s in policy.keys():\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "    # loop through all possible actions to find the best current action\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      grid.set_state(s)\n",
    "      r = grid.move(a)\n",
    "      v = r + GAMMA * V[grid.current_state()]\n",
    "      if v > best_value:\n",
    "        best_value = v\n",
    "        best_a = a\n",
    "    policy[s] = best_a\n",
    "\n",
    "  # our goal here is to verify that we get the same answer as with policy iteration\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that our rewards match up with our grid values and pre-defined gamma value (1.0 - 0.9 = -0.1). The initial random policy performed miserably, but after our value iteartion we can see that the value iteration process arrived at an optimal policy that correctly values each state. It is also fun to notice that by discounting future values at a more intense rate (0.9) we can see tha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
