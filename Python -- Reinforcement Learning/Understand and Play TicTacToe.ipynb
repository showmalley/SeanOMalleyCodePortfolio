{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Tac Toe\n",
    "## Reinforcement Learning and Discrete Optimization\n",
    "### Sean O'Malley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reinforcement Learning :__\n",
    "\n",
    "* When thinking about what reinforecment learning is, think of 3 things\n",
    "    1. An __agent__ takes actions, this is the algorithm\n",
    "    2. An __action__ is the set of possible moves\n",
    "    3. The __discount factor__ is multiplied with future rewards found by the agent to dampen the effect of the agent's choice actions.\n",
    "    4. A __state__ is a concrete and immediate condition where the agent finds itself\n",
    "    5. An __environment__ is the surroundings through which the agent moves\n",
    "    6. A __reward__ is the feedback by which we measure success\n",
    "    7. The __policy__ is the strategy the agent employes to determine the next action\n",
    "    8. The __value__ is the expected long term return with discount\n",
    "    9. The __q-value__ refers to the reward in a reinforcement problem\n",
    "    \n",
    "\n",
    "__Game Theory :__\n",
    "\n",
    "* Design the participant's decision making optimizing in order to obtain the maximum utility\n",
    "\n",
    "* Tic tac toe is an example of a sequential game, meaning players take turns and each move and every subsequent move changes the entire state of the environment.\n",
    "\n",
    "__Object Oriented Programming :__\n",
    "\n",
    "* Due to the completely separate nature of the players and the environment in which they are playing, we use object oriented programming. \n",
    "\n",
    "* Declaring an agent (player) and environment class. \n",
    "\n",
    "* Neither the agent or the environment knows what the other is doing, the agent is simply required to make choices based on the results of environment states.\n",
    "\n",
    "__Discrete Optimization :__\n",
    "\n",
    "* Variable coices are are reduced to simple categorical values, usually abstracted to integers.\n",
    "\n",
    "* Two main types:\n",
    "    * Combinatorial optimization -- optimal object from a set of finite objects\n",
    "    * Integer programming -- variables are restricted to integers\n",
    "\n",
    "__Epsilon Greedy :__\n",
    "\n",
    "* Optimization algorithm that selects the action with the highest value\n",
    "\n",
    "* _Epsilon_ is the proportion of time in which you want to be greedy (choose the best option given what we know), while _1 - Epsilon_ randomly iterates through the other options available to explore choice effectiveness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What we are using :__\n",
    "\n",
    "Simple reinforcement learning algorithm for learning tic-tac-toe\n",
    " \n",
    " \n",
    "\\begin{align}\n",
    "\\ V(s) = V(s) + alpha*(V(s') - V(s)) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Use the epsilon-greedy policy:\n",
    "   * action|s = argmax[over all actions possible from state s]{ V(s) }  if rand > epsilon\n",
    "   * action|s = select random action from possible actions from state s if rand < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions:__\n",
    "\n",
    "Currently, both agents use the same learning strategy while they play against each other.\n",
    "* What if they have different learning rates?\n",
    "* What if they have different epsilons? (probability of exploring)\n",
    "    * Who will converge faster?\n",
    "* What if one agent doesn't learn at all?\n",
    "    * Poses an interesting philosophical question: If there's no one around to challenge you,\n",
    "    * Can you reach your maximum potential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Original Code__: \n",
    "* computer goes first\n",
    "* epsilon of 0.1 means that it explores other options besides the best option 10% of the time\n",
    "* alpha of 0.5 is our learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "  def __init__(self, eps=0.1, alpha=0.5):\n",
    "    self.eps = eps # probability of choosing random action instead of greedy\n",
    "    self.alpha = alpha # learning rate\n",
    "    self.verbose = False\n",
    "    self.state_history = []\n",
    "  \n",
    "  def setV(self, V):\n",
    "    self.V = V\n",
    "\n",
    "  def set_symbol(self, sym):\n",
    "    self.sym = sym\n",
    "\n",
    "  def set_verbose(self, v):\n",
    "    # if true, will print values for each position on the board\n",
    "    self.verbose = v\n",
    "\n",
    "  def reset_history(self):\n",
    "    self.state_history = []\n",
    "\n",
    "  def take_action(self, env):\n",
    "    # choose an action based on epsilon-greedy strategy\n",
    "    r = np.random.rand()\n",
    "    best_state = None\n",
    "    if r < self.eps:\n",
    "      # take a random action\n",
    "      if self.verbose:\n",
    "        print(\"Taking a random action\")\n",
    "\n",
    "      possible_moves = []\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            possible_moves.append((i, j))\n",
    "      idx = np.random.choice(len(possible_moves))\n",
    "      next_move = possible_moves[idx]\n",
    "    else:\n",
    "      # choose the best action based on current values of states\n",
    "      # loop through all possible moves, get their values\n",
    "      # keep track of the best value\n",
    "      pos2value = {} # for debugging\n",
    "      next_move = None\n",
    "      best_value = -1\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            # what is the state if we made this move?\n",
    "            env.board[i,j] = self.sym\n",
    "            state = env.get_state()\n",
    "            env.board[i,j] = 0 # don't forget to change it back!\n",
    "            pos2value[(i,j)] = self.V[state]\n",
    "            if self.V[state] > best_value:\n",
    "              best_value = self.V[state]\n",
    "              best_state = state\n",
    "              next_move = (i, j)\n",
    "\n",
    "      # if verbose, draw the board w/ the values\n",
    "      if self.verbose:\n",
    "        print(\"Taking a greedy action\")\n",
    "        for i in range(LENGTH):\n",
    "          print(\"------------------\")\n",
    "          for j in range(LENGTH):\n",
    "            if env.is_empty(i, j):\n",
    "              # print the value\n",
    "              print(\" %.2f|\" % pos2value[(i,j)], end=\"\")\n",
    "            else:\n",
    "              print(\"  \", end=\"\")\n",
    "              if env.board[i,j] == env.x:\n",
    "                print(\"x  |\", end=\"\")\n",
    "              elif env.board[i,j] == env.o:\n",
    "                print(\"o  |\", end=\"\")\n",
    "              else:\n",
    "                print(\"   |\", end=\"\")\n",
    "          print(\"\")\n",
    "        print(\"------------------\")\n",
    "\n",
    "    # make the move\n",
    "    env.board[next_move[0], next_move[1]] = self.sym\n",
    "\n",
    "  def update_state_history(self, s):\n",
    "    # cannot put this in take_action, because take_action only happens\n",
    "    # once every other iteration for each player\n",
    "    # state history needs to be updated every iteration\n",
    "    # s = env.get_state() # don't want to do this twice so pass it in\n",
    "    self.state_history.append(s)\n",
    "\n",
    "  def update(self, env):\n",
    "    # we want to BACKTRACK over the states, so that:\n",
    "    # V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state))\n",
    "    # where V(next_state) = reward if it's the most current state\n",
    "    #\n",
    "    # NOTE: we ONLY do this at the end of an episode\n",
    "    # not so for all the algorithms we will study\n",
    "    reward = env.reward(self.sym)\n",
    "    target = reward\n",
    "    for prev in reversed(self.state_history):\n",
    "      value = self.V[prev] + self.alpha*(target - self.V[prev])\n",
    "      self.V[prev] = value\n",
    "      target = value\n",
    "    self.reset_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class represents a tic-tac-toe game\n",
    "# is a CS101-type of project\n",
    "class Environment:\n",
    "  def __init__(self):\n",
    "    self.board = np.zeros((LENGTH, LENGTH))\n",
    "    self.x = -1 # represents an x on the board, player 1\n",
    "    self.o = 1 # represents an o on the board, player 2\n",
    "    self.winner = None\n",
    "    self.ended = False\n",
    "    self.num_states = 3**(LENGTH*LENGTH)\n",
    "\n",
    "  def is_empty(self, i, j):\n",
    "    return self.board[i,j] == 0\n",
    "\n",
    "  def reward(self, sym):\n",
    "    # no reward until game is over\n",
    "    if not self.game_over():\n",
    "      return 0\n",
    "\n",
    "    # if we get here, game is over\n",
    "    # sym will be self.x or self.o\n",
    "    return 1 if self.winner == sym else 0\n",
    "\n",
    "  def get_state(self):\n",
    "    # returns the current state, represented as an int\n",
    "    # from 0...|S|-1, where S = set of all possible states\n",
    "    # |S| = 3^(BOARD SIZE), since each cell can have 3 possible values - empty, x, o\n",
    "    # some states are not possible, e.g. all cells are x, but we ignore that detail\n",
    "    # this is like finding the integer represented by a base-3 number\n",
    "    k = 0\n",
    "    h = 0\n",
    "    for i in range(LENGTH):\n",
    "      for j in range(LENGTH):\n",
    "        if self.board[i,j] == 0:\n",
    "          v = 0\n",
    "        elif self.board[i,j] == self.x:\n",
    "          v = 1\n",
    "        elif self.board[i,j] == self.o:\n",
    "          v = 2\n",
    "        h += (3**k) * v\n",
    "        k += 1\n",
    "    return h\n",
    "\n",
    "  def game_over(self, force_recalculate=False):\n",
    "    # returns true if game over (a player has won or it's a draw)\n",
    "    # otherwise returns false\n",
    "    # also sets 'winner' instance variable and 'ended' instance variable\n",
    "    if not force_recalculate and self.ended:\n",
    "      return self.ended\n",
    "    \n",
    "    # check rows\n",
    "    for i in range(LENGTH):\n",
    "      for player in (self.x, self.o):\n",
    "        if self.board[i].sum() == player*LENGTH:\n",
    "          self.winner = player\n",
    "          self.ended = True\n",
    "          return True\n",
    "\n",
    "    # check columns\n",
    "    for j in range(LENGTH):\n",
    "      for player in (self.x, self.o):\n",
    "        if self.board[:,j].sum() == player*LENGTH:\n",
    "          self.winner = player\n",
    "          self.ended = True\n",
    "          return True\n",
    "\n",
    "    # check diagonals\n",
    "    for player in (self.x, self.o):\n",
    "      # top-left -> bottom-right diagonal\n",
    "      if self.board.trace() == player*LENGTH:\n",
    "        self.winner = player\n",
    "        self.ended = True\n",
    "        return True\n",
    "      # top-right -> bottom-left diagonal\n",
    "      if np.fliplr(self.board).trace() == player*LENGTH:\n",
    "        self.winner = player\n",
    "        self.ended = True\n",
    "        return True\n",
    "\n",
    "    # check if draw\n",
    "    if np.all((self.board == 0) == False):\n",
    "      # winner stays None\n",
    "      self.winner = None\n",
    "      self.ended = True\n",
    "      return True\n",
    "\n",
    "    # game is not over\n",
    "    self.winner = None\n",
    "    return False\n",
    "\n",
    "  def is_draw(self):\n",
    "    return self.ended and self.winner is None\n",
    "\n",
    "  # Example board\n",
    "  # -------------\n",
    "  # | x |   |   |\n",
    "  # -------------\n",
    "  # |   |   |   |\n",
    "  # -------------\n",
    "  # |   |   | o |\n",
    "  # -------------\n",
    "  def draw_board(self):\n",
    "    for i in range(LENGTH):\n",
    "      print(\"-------------\")\n",
    "      for j in range(LENGTH):\n",
    "        print(\"  \", end=\"\")\n",
    "        if self.board[i,j] == self.x:\n",
    "          print(\"x \", end=\"\")\n",
    "        elif self.board[i,j] == self.o:\n",
    "          print(\"o \", end=\"\")\n",
    "        else:\n",
    "          print(\"  \", end=\"\")\n",
    "      print(\"\")\n",
    "    print(\"-------------\")\n",
    "\n",
    "\n",
    "\n",
    "class Human:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def set_symbol(self, sym):\n",
    "    self.sym = sym\n",
    "\n",
    "  def take_action(self, env):\n",
    "    while True:\n",
    "      # break if we make a legal move\n",
    "      move = input(\"Enter coordinates i,j for your next move (i,j=0..2): \")\n",
    "      i, j = move.split(',')\n",
    "      i = int(i)\n",
    "      j = int(j)\n",
    "      if env.is_empty(i, j):\n",
    "        env.board[i,j] = self.sym\n",
    "        break\n",
    "\n",
    "  def update(self, env):\n",
    "    pass\n",
    "\n",
    "  def update_state_history(self, s):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive function that will return all\n",
    "# possible states (as ints) and who the corresponding winner is for those states (if any)\n",
    "# (i, j) refers to the next cell on the board to permute (we need to try -1, 0, 1)\n",
    "# impossible games are ignored, i.e. 3x's and 3o's in a row simultaneously\n",
    "# since that will never happen in a real game\n",
    "def get_state_hash_and_winner(env, i=0, j=0):\n",
    "  results = []\n",
    "\n",
    "  for v in (0, env.x, env.o):\n",
    "    env.board[i,j] = v # if empty board it should already be 0\n",
    "    if j == 2:\n",
    "      # j goes back to 0, increase i, unless i = 2, then we are done\n",
    "      if i == 2:\n",
    "        # the board is full, collect results and return\n",
    "        state = env.get_state()\n",
    "        ended = env.game_over(force_recalculate=True)\n",
    "        winner = env.winner\n",
    "        results.append((state, winner, ended))\n",
    "      else:\n",
    "        results += get_state_hash_and_winner(env, i + 1, 0)\n",
    "    else:\n",
    "      # increment j, i stays the same\n",
    "      results += get_state_hash_and_winner(env, i, j + 1)\n",
    "\n",
    "  return results\n",
    "\n",
    "# play all possible games\n",
    "# need to also store if game is over or not\n",
    "# because we are going to initialize those values to 0.5\n",
    "# NOTE: THIS IS SLOW because MANY possible games lead to the same outcome / state\n",
    "# def get_state_hash_and_winner(env, turn='x'):\n",
    "#   results = []\n",
    "\n",
    "#   state = env.get_state()\n",
    "#   # board_before = env.board.copy()\n",
    "#   ended = env.game_over(force_recalculate=True)\n",
    "#   winner = env.winner\n",
    "#   results.append((state, winner, ended))\n",
    "\n",
    "#   # DEBUG\n",
    "#   # if ended:\n",
    "#   #   if winner is not None and env.win_type.startswith('col'):\n",
    "#   #     env.draw_board()\n",
    "#   #     print \"Winner:\", 'x' if winner == -1 else 'o', env.win_type\n",
    "#   #     print \"\\n\\n\"\n",
    "#   #     assert(np.all(board_before == env.board))\n",
    "\n",
    "#   if not ended:\n",
    "#     if turn == 'x':\n",
    "#       sym = env.x\n",
    "#       next_sym = 'o'\n",
    "#     else:\n",
    "#       sym = env.o\n",
    "#       next_sym = 'x'\n",
    "\n",
    "#     for i in xrange(LENGTH):\n",
    "#       for j in xrange(LENGTH):\n",
    "#         if env.is_empty(i, j):\n",
    "#           env.board[i,j] = sym\n",
    "#           results += get_state_hash_and_winner(env, next_sym)\n",
    "#           env.board[i,j] = 0 # reset it\n",
    "#   return results\n",
    "\n",
    "\n",
    "def initialV_x(env, state_winner_triples):\n",
    "  # initialize state values as follows\n",
    "  # if x wins, V(s) = 1\n",
    "  # if x loses or draw, V(s) = 0\n",
    "  # otherwise, V(s) = 0.5\n",
    "  V = np.zeros(env.num_states)\n",
    "  for state, winner, ended in state_winner_triples:\n",
    "    if ended:\n",
    "      if winner == env.x:\n",
    "        v = 1\n",
    "      else:\n",
    "        v = 0\n",
    "    else:\n",
    "      v = 0.5\n",
    "    V[state] = v\n",
    "  return V\n",
    "\n",
    "\n",
    "def initialV_o(env, state_winner_triples):\n",
    "  # this is (almost) the opposite of initial V for player x\n",
    "  # since everywhere where x wins (1), o loses (0)\n",
    "  # but a draw is still 0 for o\n",
    "  V = np.zeros(env.num_states)\n",
    "  for state, winner, ended in state_winner_triples:\n",
    "    if ended:\n",
    "      if winner == env.o:\n",
    "        v = 1\n",
    "      else:\n",
    "        v = 0\n",
    "    else:\n",
    "      v = 0.5\n",
    "    V[state] = v\n",
    "  return V\n",
    "\n",
    "\n",
    "def play_game(p1, p2, env, draw=False):\n",
    "  # loops until the game is over\n",
    "  current_player = None\n",
    "  while not env.game_over():\n",
    "    # alternate between players\n",
    "    # p1 always starts first\n",
    "    if current_player == p1:\n",
    "      current_player = p2\n",
    "    else:\n",
    "      current_player = p1\n",
    "\n",
    "    # draw the board before the user who wants to see it makes a move\n",
    "    if draw:\n",
    "      if draw == 1 and current_player == p1:\n",
    "        env.draw_board()\n",
    "      if draw == 2 and current_player == p2:\n",
    "        env.draw_board()\n",
    "\n",
    "    # current player makes a move\n",
    "    current_player.take_action(env)\n",
    "\n",
    "    # update state histories\n",
    "    state = env.get_state()\n",
    "    p1.update_state_history(state)\n",
    "    p2.update_state_history(state)\n",
    "\n",
    "  if draw:\n",
    "    env.draw_board()\n",
    "\n",
    "  # do the value function update\n",
    "  p1.update(env)\n",
    "  p2.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.58| 0.52| 0.77|\n",
      "------------------\n",
      " 0.70| 0.77| 0.77|\n",
      "------------------\n",
      " 0.67| 0.71| 0.56|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "          x \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.53| 0.50| 0.50|\n",
      "------------------\n",
      "  o  | 0.62|  x  |\n",
      "------------------\n",
      " 0.50| 0.50| 0.83|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "  o       x \n",
      "-------------\n",
      "          x \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.50| 0.50| 1.00|\n",
      "------------------\n",
      "  o  | 0.03|  x  |\n",
      "------------------\n",
      "  o  | 0.50|  x  |\n",
      "------------------\n",
      "-------------\n",
      "          x \n",
      "-------------\n",
      "  o       x \n",
      "-------------\n",
      "  o       x \n",
      "-------------\n",
      "Play again? [Y/n]: Y\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.58| 0.52| 0.77|\n",
      "------------------\n",
      " 0.70| 0.77| 0.79|\n",
      "------------------\n",
      " 0.67| 0.71| 0.56|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "          x \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.45| 0.44| 0.70|\n",
      "------------------\n",
      " 0.50| 0.85|  x  |\n",
      "------------------\n",
      "  o  | 0.50| 0.50|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x   x \n",
      "-------------\n",
      "  o         \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.94| 0.25| 0.06|\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      "  o  | 0.25| 0.03|\n",
      "------------------\n",
      "-------------\n",
      "  x         \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "  o         \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  x  | 0.02| 0.03|\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      "  o  | 0.37|  o  |\n",
      "------------------\n",
      "-------------\n",
      "  x         \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "  o   x   o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,0\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  x  |  o  | 0.00|\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      "  o  |  x  |  o  |\n",
      "------------------\n",
      "-------------\n",
      "  x   o   x \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "  o   x   o \n",
      "-------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/Python36/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/Python36/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/Python36/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/Python36/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e615e4d881ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# select the center as its starting move. If you want the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# to go second you can switch the human and AI.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Play again? [Y/n]: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/Python36/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/Python36/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  # train the agent\n",
    "  p1 = Agent()\n",
    "  p2 = Agent()\n",
    "\n",
    "  # set initial V for p1 and p2\n",
    "  env = Environment()\n",
    "  state_winner_triples = get_state_hash_and_winner(env)\n",
    "\n",
    "\n",
    "  Vx = initialV_x(env, state_winner_triples)\n",
    "  p1.setV(Vx)\n",
    "  Vo = initialV_o(env, state_winner_triples)\n",
    "  p2.setV(Vo)\n",
    "\n",
    "  # give each player their symbol\n",
    "  p1.set_symbol(env.x)\n",
    "  p2.set_symbol(env.o)\n",
    "\n",
    "  T = 10000\n",
    "  for t in range(T):\n",
    "    if t % 200 == 0:\n",
    "      print(t)\n",
    "    play_game(p1, p2, Environment())\n",
    "\n",
    "  # play human vs. agent\n",
    "  # do you think the agent learned to play the game well?\n",
    "  human = Human()\n",
    "  human.set_symbol(env.o)\n",
    "  while True:\n",
    "    p1.set_verbose(True)\n",
    "    play_game(p1, human, Environment(), draw=2)\n",
    "    # I made the agent player 1 because I wanted to see if it would\n",
    "    # select the center as its starting move. If you want the agent\n",
    "    # to go second you can switch the human and AI.\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the game and I got in a draw and it played really well\n",
    "\n",
    "__High Epsilon__:\n",
    "\n",
    "Lets see how setting explore to a much higher value effects the game's ability to play. Lets make the adjustment to the agent then play the game again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2:\n",
    "  def __init__(self, eps= 0.4, alpha = 0.5):\n",
    "    self.eps = eps # probability of choosing random action instead of greedy\n",
    "    self.alpha = alpha # learning rate\n",
    "    self.verbose = False\n",
    "    self.state_history = []\n",
    "  \n",
    "  def setV(self, V):\n",
    "    self.V = V\n",
    "\n",
    "  def set_symbol(self, sym):\n",
    "    self.sym = sym\n",
    "\n",
    "  def set_verbose(self, v):\n",
    "    # if true, will print values for each position on the board\n",
    "    self.verbose = v\n",
    "\n",
    "  def reset_history(self):\n",
    "    self.state_history = []\n",
    "\n",
    "  def take_action(self, env):\n",
    "    # choose an action based on epsilon-greedy strategy\n",
    "    r = np.random.rand()\n",
    "    best_state = None\n",
    "    if r < self.eps:\n",
    "      # take a random action\n",
    "      if self.verbose:\n",
    "        print(\"Taking a random action\")\n",
    "\n",
    "      possible_moves = []\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            possible_moves.append((i, j))\n",
    "      idx = np.random.choice(len(possible_moves))\n",
    "      next_move = possible_moves[idx]\n",
    "    else:\n",
    "      # choose the best action based on current values of states\n",
    "      # loop through all possible moves, get their values\n",
    "      # keep track of the best value\n",
    "      pos2value = {} # for debugging\n",
    "      next_move = None\n",
    "      best_value = -1\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            # what is the state if we made this move?\n",
    "            env.board[i,j] = self.sym\n",
    "            state = env.get_state()\n",
    "            env.board[i,j] = 0 # don't forget to change it back!\n",
    "            pos2value[(i,j)] = self.V[state]\n",
    "            if self.V[state] > best_value:\n",
    "              best_value = self.V[state]\n",
    "              best_state = state\n",
    "              next_move = (i, j)\n",
    "\n",
    "      # if verbose, draw the board w/ the values\n",
    "      if self.verbose:\n",
    "        print(\"Taking a greedy action\")\n",
    "        for i in range(LENGTH):\n",
    "          print(\"------------------\")\n",
    "          for j in range(LENGTH):\n",
    "            if env.is_empty(i, j):\n",
    "              # print the value\n",
    "              print(\" %.2f|\" % pos2value[(i,j)], end=\"\")\n",
    "            else:\n",
    "              print(\"  \", end=\"\")\n",
    "              if env.board[i,j] == env.x:\n",
    "                print(\"x  |\", end=\"\")\n",
    "              elif env.board[i,j] == env.o:\n",
    "                print(\"o  |\", end=\"\")\n",
    "              else:\n",
    "                print(\"   |\", end=\"\")\n",
    "          print(\"\")\n",
    "        print(\"------------------\")\n",
    "\n",
    "    # make the move\n",
    "    env.board[next_move[0], next_move[1]] = self.sym\n",
    "\n",
    "  def update_state_history(self, s):\n",
    "    # cannot put this in take_action, because take_action only happens\n",
    "    # once every other iteration for each player\n",
    "    # state history needs to be updated every iteration\n",
    "    # s = env.get_state() # don't want to do this twice so pass it in\n",
    "    self.state_history.append(s)\n",
    "\n",
    "  def update(self, env):\n",
    "    # we want to BACKTRACK over the states, so that:\n",
    "    # V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state))\n",
    "    # where V(next_state) = reward if it's the most current state\n",
    "    #\n",
    "    # NOTE: we ONLY do this at the end of an episode\n",
    "    # not so for all the algorithms we will study\n",
    "    reward = env.reward(self.sym)\n",
    "    target = reward\n",
    "    for prev in reversed(self.state_history):\n",
    "      value = self.V[prev] + self.alpha*(target - self.V[prev])\n",
    "      self.V[prev] = value\n",
    "      target = value\n",
    "    self.reset_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.62| 0.45| 0.34|\n",
      "------------------\n",
      " 0.52| 0.87| 0.50|\n",
      "------------------\n",
      " 0.61| 0.67| 0.67|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.83| 0.76| 0.47|\n",
      "------------------\n",
      " 0.80|  x  | 0.76|\n",
      "------------------\n",
      "  o  | 0.95| 0.75|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.78|  o  | 0.54|\n",
      "------------------\n",
      " 0.59|  x  | 0.60|\n",
      "------------------\n",
      "  o  |  x  | 0.02|\n",
      "------------------\n",
      "-------------\n",
      "  x   o     \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "Taking a random action\n",
      "-------------\n",
      "  x   o     \n",
      "-------------\n",
      "      x   x \n",
      "-------------\n",
      "  o   x   o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  x  |  o  | 0.00|\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      "  o  |  x  |  o  |\n",
      "------------------\n",
      "-------------\n",
      "  x   o   x \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "  o   x   o \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  # train the agent\n",
    "  p1 = Agent2()\n",
    "  p2 = Agent2()\n",
    "\n",
    "  # set initial V for p1 and p2\n",
    "  env = Environment()\n",
    "  state_winner_triples = get_state_hash_and_winner(env)\n",
    "\n",
    "\n",
    "  Vx = initialV_x(env, state_winner_triples)\n",
    "  p1.setV(Vx)\n",
    "  Vo = initialV_o(env, state_winner_triples)\n",
    "  p2.setV(Vo)\n",
    "\n",
    "  # give each player their symbol\n",
    "  p1.set_symbol(env.x)\n",
    "  p2.set_symbol(env.o)\n",
    "\n",
    "  T = 10000\n",
    "  for t in range(T):\n",
    "    if t % 200 == 0:\n",
    "      print(t)\n",
    "    play_game(p1, p2, Environment())\n",
    "\n",
    "  # play human vs. agent\n",
    "  # do you think the agent learned to play the game well?\n",
    "  human = Human()\n",
    "  human.set_symbol(env.o)\n",
    "  while True:\n",
    "    p1.set_verbose(True)\n",
    "    play_game(p1, human, Environment(), draw=2)\n",
    "    # I made the agent player 1 because I wanted to see if it would\n",
    "    # select the center as its starting move. If you want the agent\n",
    "    # to go second you can switch the human and AI.\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off the bat on the first move we can see that the confidence in any other moves are much lower than the previous iteration, but in playing the AI it played in a much more skilled manner than the previous iteration, requiring me to play perfect defense to win. \n",
    "\n",
    "Given that the algorithm has really played well on all accounts, for the sake of my pride, lets lower the learning rate and see if I can beat it then.\n",
    "\n",
    "__Low Alpha:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent3:\n",
    "  def __init__(self, eps= 0.1, alpha = 0.2):\n",
    "    self.eps = eps # probability of choosing random action instead of greedy\n",
    "    self.alpha = alpha # learning rate\n",
    "    self.verbose = False\n",
    "    self.state_history = []\n",
    "  \n",
    "  def setV(self, V):\n",
    "    self.V = V\n",
    "\n",
    "  def set_symbol(self, sym):\n",
    "    self.sym = sym\n",
    "\n",
    "  def set_verbose(self, v):\n",
    "    # if true, will print values for each position on the board\n",
    "    self.verbose = v\n",
    "\n",
    "  def reset_history(self):\n",
    "    self.state_history = []\n",
    "\n",
    "  def take_action(self, env):\n",
    "    # choose an action based on epsilon-greedy strategy\n",
    "    r = np.random.rand()\n",
    "    best_state = None\n",
    "    if r < self.eps:\n",
    "      # take a random action\n",
    "      if self.verbose:\n",
    "        print(\"Taking a random action\")\n",
    "\n",
    "      possible_moves = []\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            possible_moves.append((i, j))\n",
    "      idx = np.random.choice(len(possible_moves))\n",
    "      next_move = possible_moves[idx]\n",
    "    else:\n",
    "      # choose the best action based on current values of states\n",
    "      # loop through all possible moves, get their values\n",
    "      # keep track of the best value\n",
    "      pos2value = {} # for debugging\n",
    "      next_move = None\n",
    "      best_value = -1\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            # what is the state if we made this move?\n",
    "            env.board[i,j] = self.sym\n",
    "            state = env.get_state()\n",
    "            env.board[i,j] = 0 # don't forget to change it back!\n",
    "            pos2value[(i,j)] = self.V[state]\n",
    "            if self.V[state] > best_value:\n",
    "              best_value = self.V[state]\n",
    "              best_state = state\n",
    "              next_move = (i, j)\n",
    "\n",
    "      # if verbose, draw the board w/ the values\n",
    "      if self.verbose:\n",
    "        print(\"Taking a greedy action\")\n",
    "        for i in range(LENGTH):\n",
    "          print(\"------------------\")\n",
    "          for j in range(LENGTH):\n",
    "            if env.is_empty(i, j):\n",
    "              # print the value\n",
    "              print(\" %.2f|\" % pos2value[(i,j)], end=\"\")\n",
    "            else:\n",
    "              print(\"  \", end=\"\")\n",
    "              if env.board[i,j] == env.x:\n",
    "                print(\"x  |\", end=\"\")\n",
    "              elif env.board[i,j] == env.o:\n",
    "                print(\"o  |\", end=\"\")\n",
    "              else:\n",
    "                print(\"   |\", end=\"\")\n",
    "          print(\"\")\n",
    "        print(\"------------------\")\n",
    "\n",
    "    # make the move\n",
    "    env.board[next_move[0], next_move[1]] = self.sym\n",
    "\n",
    "  def update_state_history(self, s):\n",
    "    # cannot put this in take_action, because take_action only happens\n",
    "    # once every other iteration for each player\n",
    "    # state history needs to be updated every iteration\n",
    "    # s = env.get_state() # don't want to do this twice so pass it in\n",
    "    self.state_history.append(s)\n",
    "\n",
    "  def update(self, env):\n",
    "    # we want to BACKTRACK over the states, so that:\n",
    "    # V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state))\n",
    "    # where V(next_state) = reward if it's the most current state\n",
    "    #\n",
    "    # NOTE: we ONLY do this at the end of an episode\n",
    "    # not so for all the algorithms we will study\n",
    "    reward = env.reward(self.sym)\n",
    "    target = reward\n",
    "    for prev in reversed(self.state_history):\n",
    "      value = self.V[prev] + self.alpha*(target - self.V[prev])\n",
    "      self.V[prev] = value\n",
    "      target = value\n",
    "    self.reset_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.50| 0.49| 0.50|\n",
      "------------------\n",
      " 0.49| 0.92| 0.49|\n",
      "------------------\n",
      " 0.53| 0.51| 0.53|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.79| 0.63| 0.67|\n",
      "------------------\n",
      " 0.58|  x  | 0.88|\n",
      "------------------\n",
      " 0.75| 0.67|  o  |\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x   x \n",
      "-------------\n",
      "          o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.52|  o  | 0.54|\n",
      "------------------\n",
      " 1.00|  x  |  x  |\n",
      "------------------\n",
      " 0.50| 0.61|  o  |\n",
      "------------------\n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "  x   x   x \n",
      "-------------\n",
      "          o \n",
      "-------------\n",
      "Play again? [Y/n]: Y\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.50| 0.49| 0.50|\n",
      "------------------\n",
      " 0.49| 0.91| 0.49|\n",
      "------------------\n",
      " 0.53| 0.51| 0.53|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.79| 0.63| 0.67|\n",
      "------------------\n",
      " 0.58|  x  | 0.91|\n",
      "------------------\n",
      " 0.75| 0.67|  o  |\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x   x \n",
      "-------------\n",
      "          o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.52|  o  | 0.54|\n",
      "------------------\n",
      " 1.00|  x  |  x  |\n",
      "------------------\n",
      " 0.50| 0.61|  o  |\n",
      "------------------\n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "  x   x   x \n",
      "-------------\n",
      "          o \n",
      "-------------\n",
      "Play again? [Y/n]: \n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.50| 0.49| 0.50|\n",
      "------------------\n",
      " 0.49| 0.91| 0.49|\n",
      "------------------\n",
      " 0.53| 0.51| 0.53|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.79| 0.63| 0.67|\n",
      "------------------\n",
      " 0.58|  x  | 0.92|\n",
      "------------------\n",
      " 0.75| 0.67|  o  |\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x   x \n",
      "-------------\n",
      "          o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.21| 0.76| 0.27|\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      " 0.62| 0.65|  o  |\n",
      "------------------\n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "          o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.16|  x  | 0.32|\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      " 0.36|  o  |  o  |\n",
      "------------------\n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "  x   o   o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.00|  x  |  o  |\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      "  x  |  o  |  o  |\n",
      "------------------\n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "  x   o   o \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  # train the agent\n",
    "  p1 = Agent3()\n",
    "  p2 = Agent3()\n",
    "\n",
    "  # set initial V for p1 and p2\n",
    "  env = Environment()\n",
    "  state_winner_triples = get_state_hash_and_winner(env)\n",
    "\n",
    "\n",
    "  Vx = initialV_x(env, state_winner_triples)\n",
    "  p1.setV(Vx)\n",
    "  Vo = initialV_o(env, state_winner_triples)\n",
    "  p2.setV(Vo)\n",
    "\n",
    "  # give each player their symbol\n",
    "  p1.set_symbol(env.x)\n",
    "  p2.set_symbol(env.o)\n",
    "\n",
    "  T = 10000\n",
    "  for t in range(T):\n",
    "    if t % 200 == 0:\n",
    "      print(t)\n",
    "    play_game(p1, p2, Environment())\n",
    "\n",
    "  # play human vs. agent\n",
    "  # do you think the agent learned to play the game well?\n",
    "  human = Human()\n",
    "  human.set_symbol(env.o)\n",
    "  while True:\n",
    "    p1.set_verbose(True)\n",
    "    play_game(p1, human, Environment(), draw=2)\n",
    "    # I made the agent player 1 because I wanted to see if it would\n",
    "    # select the center as its starting move. If you want the agent\n",
    "    # to go second you can switch the human and AI.\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there had been a sufficient amount of learning up to this point. I actually gave the AI a chance to beat me and it immediately took it, and then in trying to beat the AI, I still couldn't get it done. \n",
    "\n",
    "Now, lets return to the first agent and select who goes first.\n",
    "\n",
    "__I go first:__\n",
    "\n",
    "* Note, some code changes that hat to occur here is that we are now playing p2, which is trained on the second player choices. We also changed when we drew the board and swapped the human and computer objects within the play_game function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,1\n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "-------------\n",
      "  o   o     \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "          x \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,2\n",
      "-------------\n",
      "  o   o   x \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  o       x \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,2\n",
      "-------------\n",
      "  o   o   x \n",
      "-------------\n",
      "      x   x \n",
      "-------------\n",
      "  o       x \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  # train the agent\n",
    "  p1 = Agent()\n",
    "  p2 = Agent()\n",
    "\n",
    "  # set initial V for p1 and p2\n",
    "  env = Environment()\n",
    "  state_winner_triples = get_state_hash_and_winner(env)\n",
    "\n",
    "\n",
    "  Vx = initialV_x(env, state_winner_triples)\n",
    "  p1.setV(Vx)\n",
    "  Vo = initialV_o(env, state_winner_triples)\n",
    "  p2.setV(Vo)\n",
    "\n",
    "  # give each player their symbol\n",
    "  p1.set_symbol(env.x)\n",
    "  p2.set_symbol(env.o)\n",
    "\n",
    "  T = 10000\n",
    "  for t in range(T):\n",
    "    if t % 200 == 0:\n",
    "      print(t)\n",
    "    play_game(p1, p2, Environment())\n",
    "\n",
    "  # play human vs. agent\n",
    "  # do you think the agent learned to play the game well?\n",
    "  human = Human()\n",
    "  human.set_symbol(env.x)\n",
    "  while True:\n",
    "    p1.set_verbose(True)\n",
    "    play_game(human, p2, Environment(), draw=1)\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My ego has shot through the roof. Take that AI, who is the smart one now! \n",
    "\n",
    "Now that we know the game is beatable with me going first, lets try and create the smartest agent possible.\n",
    "\n",
    "__Smart Agent:__\n",
    "\n",
    "we are raising epsilon, lowering alpha but extending the training time by a factor of 10, then we'll play again and see if we can beat the AI this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent4:\n",
    "  def __init__(self, eps= 0.2, alpha = 0.5):\n",
    "    self.eps = eps # probability of choosing random action instead of greedy\n",
    "    self.alpha = alpha # learning rate\n",
    "    self.verbose = False\n",
    "    self.state_history = []\n",
    "  \n",
    "  def setV(self, V):\n",
    "    self.V = V\n",
    "\n",
    "  def set_symbol(self, sym):\n",
    "    self.sym = sym\n",
    "\n",
    "  def set_verbose(self, v):\n",
    "    # if true, will print values for each position on the board\n",
    "    self.verbose = v\n",
    "\n",
    "  def reset_history(self):\n",
    "    self.state_history = []\n",
    "\n",
    "  def take_action(self, env):\n",
    "    # choose an action based on epsilon-greedy strategy\n",
    "    r = np.random.rand()\n",
    "    best_state = None\n",
    "    if r < self.eps:\n",
    "      # take a random action\n",
    "      if self.verbose:\n",
    "        print(\"Taking a random action\")\n",
    "\n",
    "      possible_moves = []\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            possible_moves.append((i, j))\n",
    "      idx = np.random.choice(len(possible_moves))\n",
    "      next_move = possible_moves[idx]\n",
    "    else:\n",
    "      # choose the best action based on current values of states\n",
    "      # loop through all possible moves, get their values\n",
    "      # keep track of the best value\n",
    "      pos2value = {} # for debugging\n",
    "      next_move = None\n",
    "      best_value = -1\n",
    "      for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "          if env.is_empty(i, j):\n",
    "            # what is the state if we made this move?\n",
    "            env.board[i,j] = self.sym\n",
    "            state = env.get_state()\n",
    "            env.board[i,j] = 0 # don't forget to change it back!\n",
    "            pos2value[(i,j)] = self.V[state]\n",
    "            if self.V[state] > best_value:\n",
    "              best_value = self.V[state]\n",
    "              best_state = state\n",
    "              next_move = (i, j)\n",
    "\n",
    "      # if verbose, draw the board w/ the values\n",
    "      if self.verbose:\n",
    "        print(\"Taking a greedy action\")\n",
    "        for i in range(LENGTH):\n",
    "          print(\"------------------\")\n",
    "          for j in range(LENGTH):\n",
    "            if env.is_empty(i, j):\n",
    "              # print the value\n",
    "              print(\" %.2f|\" % pos2value[(i,j)], end=\"\")\n",
    "            else:\n",
    "              print(\"  \", end=\"\")\n",
    "              if env.board[i,j] == env.x:\n",
    "                print(\"x  |\", end=\"\")\n",
    "              elif env.board[i,j] == env.o:\n",
    "                print(\"o  |\", end=\"\")\n",
    "              else:\n",
    "                print(\"   |\", end=\"\")\n",
    "          print(\"\")\n",
    "        print(\"------------------\")\n",
    "\n",
    "    # make the move\n",
    "    env.board[next_move[0], next_move[1]] = self.sym\n",
    "\n",
    "  def update_state_history(self, s):\n",
    "    # cannot put this in take_action, because take_action only happens\n",
    "    # once every other iteration for each player\n",
    "    # state history needs to be updated every iteration\n",
    "    # s = env.get_state() # don't want to do this twice so pass it in\n",
    "    self.state_history.append(s)\n",
    "\n",
    "  def update(self, env):\n",
    "    # we want to BACKTRACK over the states, so that:\n",
    "    # V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state))\n",
    "    # where V(next_state) = reward if it's the most current state\n",
    "    #\n",
    "    # NOTE: we ONLY do this at the end of an episode\n",
    "    # not so for all the algorithms we will study\n",
    "    reward = env.reward(self.sym)\n",
    "    target = reward\n",
    "    for prev in reversed(self.state_history):\n",
    "      value = self.V[prev] + self.alpha*(target - self.V[prev])\n",
    "      self.V[prev] = value\n",
    "      target = value\n",
    "    self.reset_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n",
      "10000\n",
      "10200\n",
      "10400\n",
      "10600\n",
      "10800\n",
      "11000\n",
      "11200\n",
      "11400\n",
      "11600\n",
      "11800\n",
      "12000\n",
      "12200\n",
      "12400\n",
      "12600\n",
      "12800\n",
      "13000\n",
      "13200\n",
      "13400\n",
      "13600\n",
      "13800\n",
      "14000\n",
      "14200\n",
      "14400\n",
      "14600\n",
      "14800\n",
      "15000\n",
      "15200\n",
      "15400\n",
      "15600\n",
      "15800\n",
      "16000\n",
      "16200\n",
      "16400\n",
      "16600\n",
      "16800\n",
      "17000\n",
      "17200\n",
      "17400\n",
      "17600\n",
      "17800\n",
      "18000\n",
      "18200\n",
      "18400\n",
      "18600\n",
      "18800\n",
      "19000\n",
      "19200\n",
      "19400\n",
      "19600\n",
      "19800\n",
      "20000\n",
      "20200\n",
      "20400\n",
      "20600\n",
      "20800\n",
      "21000\n",
      "21200\n",
      "21400\n",
      "21600\n",
      "21800\n",
      "22000\n",
      "22200\n",
      "22400\n",
      "22600\n",
      "22800\n",
      "23000\n",
      "23200\n",
      "23400\n",
      "23600\n",
      "23800\n",
      "24000\n",
      "24200\n",
      "24400\n",
      "24600\n",
      "24800\n",
      "25000\n",
      "25200\n",
      "25400\n",
      "25600\n",
      "25800\n",
      "26000\n",
      "26200\n",
      "26400\n",
      "26600\n",
      "26800\n",
      "27000\n",
      "27200\n",
      "27400\n",
      "27600\n",
      "27800\n",
      "28000\n",
      "28200\n",
      "28400\n",
      "28600\n",
      "28800\n",
      "29000\n",
      "29200\n",
      "29400\n",
      "29600\n",
      "29800\n",
      "30000\n",
      "30200\n",
      "30400\n",
      "30600\n",
      "30800\n",
      "31000\n",
      "31200\n",
      "31400\n",
      "31600\n",
      "31800\n",
      "32000\n",
      "32200\n",
      "32400\n",
      "32600\n",
      "32800\n",
      "33000\n",
      "33200\n",
      "33400\n",
      "33600\n",
      "33800\n",
      "34000\n",
      "34200\n",
      "34400\n",
      "34600\n",
      "34800\n",
      "35000\n",
      "35200\n",
      "35400\n",
      "35600\n",
      "35800\n",
      "36000\n",
      "36200\n",
      "36400\n",
      "36600\n",
      "36800\n",
      "37000\n",
      "37200\n",
      "37400\n",
      "37600\n",
      "37800\n",
      "38000\n",
      "38200\n",
      "38400\n",
      "38600\n",
      "38800\n",
      "39000\n",
      "39200\n",
      "39400\n",
      "39600\n",
      "39800\n",
      "40000\n",
      "40200\n",
      "40400\n",
      "40600\n",
      "40800\n",
      "41000\n",
      "41200\n",
      "41400\n",
      "41600\n",
      "41800\n",
      "42000\n",
      "42200\n",
      "42400\n",
      "42600\n",
      "42800\n",
      "43000\n",
      "43200\n",
      "43400\n",
      "43600\n",
      "43800\n",
      "44000\n",
      "44200\n",
      "44400\n",
      "44600\n",
      "44800\n",
      "45000\n",
      "45200\n",
      "45400\n",
      "45600\n",
      "45800\n",
      "46000\n",
      "46200\n",
      "46400\n",
      "46600\n",
      "46800\n",
      "47000\n",
      "47200\n",
      "47400\n",
      "47600\n",
      "47800\n",
      "48000\n",
      "48200\n",
      "48400\n",
      "48600\n",
      "48800\n",
      "49000\n",
      "49200\n",
      "49400\n",
      "49600\n",
      "49800\n",
      "50000\n",
      "50200\n",
      "50400\n",
      "50600\n",
      "50800\n",
      "51000\n",
      "51200\n",
      "51400\n",
      "51600\n",
      "51800\n",
      "52000\n",
      "52200\n",
      "52400\n",
      "52600\n",
      "52800\n",
      "53000\n",
      "53200\n",
      "53400\n",
      "53600\n",
      "53800\n",
      "54000\n",
      "54200\n",
      "54400\n",
      "54600\n",
      "54800\n",
      "55000\n",
      "55200\n",
      "55400\n",
      "55600\n",
      "55800\n",
      "56000\n",
      "56200\n",
      "56400\n",
      "56600\n",
      "56800\n",
      "57000\n",
      "57200\n",
      "57400\n",
      "57600\n",
      "57800\n",
      "58000\n",
      "58200\n",
      "58400\n",
      "58600\n",
      "58800\n",
      "59000\n",
      "59200\n",
      "59400\n",
      "59600\n",
      "59800\n",
      "60000\n",
      "60200\n",
      "60400\n",
      "60600\n",
      "60800\n",
      "61000\n",
      "61200\n",
      "61400\n",
      "61600\n",
      "61800\n",
      "62000\n",
      "62200\n",
      "62400\n",
      "62600\n",
      "62800\n",
      "63000\n",
      "63200\n",
      "63400\n",
      "63600\n",
      "63800\n",
      "64000\n",
      "64200\n",
      "64400\n",
      "64600\n",
      "64800\n",
      "65000\n",
      "65200\n",
      "65400\n",
      "65600\n",
      "65800\n",
      "66000\n",
      "66200\n",
      "66400\n",
      "66600\n",
      "66800\n",
      "67000\n",
      "67200\n",
      "67400\n",
      "67600\n",
      "67800\n",
      "68000\n",
      "68200\n",
      "68400\n",
      "68600\n",
      "68800\n",
      "69000\n",
      "69200\n",
      "69400\n",
      "69600\n",
      "69800\n",
      "70000\n",
      "70200\n",
      "70400\n",
      "70600\n",
      "70800\n",
      "71000\n",
      "71200\n",
      "71400\n",
      "71600\n",
      "71800\n",
      "72000\n",
      "72200\n",
      "72400\n",
      "72600\n",
      "72800\n",
      "73000\n",
      "73200\n",
      "73400\n",
      "73600\n",
      "73800\n",
      "74000\n",
      "74200\n",
      "74400\n",
      "74600\n",
      "74800\n",
      "75000\n",
      "75200\n",
      "75400\n",
      "75600\n",
      "75800\n",
      "76000\n",
      "76200\n",
      "76400\n",
      "76600\n",
      "76800\n",
      "77000\n",
      "77200\n",
      "77400\n",
      "77600\n",
      "77800\n",
      "78000\n",
      "78200\n",
      "78400\n",
      "78600\n",
      "78800\n",
      "79000\n",
      "79200\n",
      "79400\n",
      "79600\n",
      "79800\n",
      "80000\n",
      "80200\n",
      "80400\n",
      "80600\n",
      "80800\n",
      "81000\n",
      "81200\n",
      "81400\n",
      "81600\n",
      "81800\n",
      "82000\n",
      "82200\n",
      "82400\n",
      "82600\n",
      "82800\n",
      "83000\n",
      "83200\n",
      "83400\n",
      "83600\n",
      "83800\n",
      "84000\n",
      "84200\n",
      "84400\n",
      "84600\n",
      "84800\n",
      "85000\n",
      "85200\n",
      "85400\n",
      "85600\n",
      "85800\n",
      "86000\n",
      "86200\n",
      "86400\n",
      "86600\n",
      "86800\n",
      "87000\n",
      "87200\n",
      "87400\n",
      "87600\n",
      "87800\n",
      "88000\n",
      "88200\n",
      "88400\n",
      "88600\n",
      "88800\n",
      "89000\n",
      "89200\n",
      "89400\n",
      "89600\n",
      "89800\n",
      "90000\n",
      "90200\n",
      "90400\n",
      "90600\n",
      "90800\n",
      "91000\n",
      "91200\n",
      "91400\n",
      "91600\n",
      "91800\n",
      "92000\n",
      "92200\n",
      "92400\n",
      "92600\n",
      "92800\n",
      "93000\n",
      "93200\n",
      "93400\n",
      "93600\n",
      "93800\n",
      "94000\n",
      "94200\n",
      "94400\n",
      "94600\n",
      "94800\n",
      "95000\n",
      "95200\n",
      "95400\n",
      "95600\n",
      "95800\n",
      "96000\n",
      "96200\n",
      "96400\n",
      "96600\n",
      "96800\n",
      "97000\n",
      "97200\n",
      "97400\n",
      "97600\n",
      "97800\n",
      "98000\n",
      "98200\n",
      "98400\n",
      "98600\n",
      "98800\n",
      "99000\n",
      "99200\n",
      "99400\n",
      "99600\n",
      "99800\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,1\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "  o       x \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,0\n",
      "-------------\n",
      "  x         \n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "  o       x \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  # train the agent\n",
    "  p1 = Agent4()\n",
    "  p2 = Agent4()\n",
    "\n",
    "  # set initial V for p1 and p2\n",
    "  env = Environment()\n",
    "  state_winner_triples = get_state_hash_and_winner(env)\n",
    "\n",
    "\n",
    "  Vx = initialV_x(env, state_winner_triples)\n",
    "  p1.setV(Vx)\n",
    "  Vo = initialV_o(env, state_winner_triples)\n",
    "  p2.setV(Vo)\n",
    "\n",
    "  # give each player their symbol\n",
    "  p1.set_symbol(env.x)\n",
    "  p2.set_symbol(env.o)\n",
    "\n",
    "  T = 100000\n",
    "  for t in range(T):\n",
    "    if t % 200 == 0:\n",
    "      print(t)\n",
    "    play_game(p1, p2, Environment())\n",
    "\n",
    "  # play human vs. agent\n",
    "  # do you think the agent learned to play the game well?\n",
    "  human = Human()\n",
    "  human.set_symbol(env.x)\n",
    "  while True:\n",
    "    p1.set_verbose(True)\n",
    "    play_game(human, p2, Environment(), draw=1)\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I still beat the AI despite increasing the training times, something I was wondering was what if we trained two different agent types against one another? Lets try!\n",
    "\n",
    "__Training Buddy:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n",
      "10000\n",
      "10200\n",
      "10400\n",
      "10600\n",
      "10800\n",
      "11000\n",
      "11200\n",
      "11400\n",
      "11600\n",
      "11800\n",
      "12000\n",
      "12200\n",
      "12400\n",
      "12600\n",
      "12800\n",
      "13000\n",
      "13200\n",
      "13400\n",
      "13600\n",
      "13800\n",
      "14000\n",
      "14200\n",
      "14400\n",
      "14600\n",
      "14800\n",
      "15000\n",
      "15200\n",
      "15400\n",
      "15600\n",
      "15800\n",
      "16000\n",
      "16200\n",
      "16400\n",
      "16600\n",
      "16800\n",
      "17000\n",
      "17200\n",
      "17400\n",
      "17600\n",
      "17800\n",
      "18000\n",
      "18200\n",
      "18400\n",
      "18600\n",
      "18800\n",
      "19000\n",
      "19200\n",
      "19400\n",
      "19600\n",
      "19800\n",
      "20000\n",
      "20200\n",
      "20400\n",
      "20600\n",
      "20800\n",
      "21000\n",
      "21200\n",
      "21400\n",
      "21600\n",
      "21800\n",
      "22000\n",
      "22200\n",
      "22400\n",
      "22600\n",
      "22800\n",
      "23000\n",
      "23200\n",
      "23400\n",
      "23600\n",
      "23800\n",
      "24000\n",
      "24200\n",
      "24400\n",
      "24600\n",
      "24800\n",
      "25000\n",
      "25200\n",
      "25400\n",
      "25600\n",
      "25800\n",
      "26000\n",
      "26200\n",
      "26400\n",
      "26600\n",
      "26800\n",
      "27000\n",
      "27200\n",
      "27400\n",
      "27600\n",
      "27800\n",
      "28000\n",
      "28200\n",
      "28400\n",
      "28600\n",
      "28800\n",
      "29000\n",
      "29200\n",
      "29400\n",
      "29600\n",
      "29800\n",
      "30000\n",
      "30200\n",
      "30400\n",
      "30600\n",
      "30800\n",
      "31000\n",
      "31200\n",
      "31400\n",
      "31600\n",
      "31800\n",
      "32000\n",
      "32200\n",
      "32400\n",
      "32600\n",
      "32800\n",
      "33000\n",
      "33200\n",
      "33400\n",
      "33600\n",
      "33800\n",
      "34000\n",
      "34200\n",
      "34400\n",
      "34600\n",
      "34800\n",
      "35000\n",
      "35200\n",
      "35400\n",
      "35600\n",
      "35800\n",
      "36000\n",
      "36200\n",
      "36400\n",
      "36600\n",
      "36800\n",
      "37000\n",
      "37200\n",
      "37400\n",
      "37600\n",
      "37800\n",
      "38000\n",
      "38200\n",
      "38400\n",
      "38600\n",
      "38800\n",
      "39000\n",
      "39200\n",
      "39400\n",
      "39600\n",
      "39800\n",
      "40000\n",
      "40200\n",
      "40400\n",
      "40600\n",
      "40800\n",
      "41000\n",
      "41200\n",
      "41400\n",
      "41600\n",
      "41800\n",
      "42000\n",
      "42200\n",
      "42400\n",
      "42600\n",
      "42800\n",
      "43000\n",
      "43200\n",
      "43400\n",
      "43600\n",
      "43800\n",
      "44000\n",
      "44200\n",
      "44400\n",
      "44600\n",
      "44800\n",
      "45000\n",
      "45200\n",
      "45400\n",
      "45600\n",
      "45800\n",
      "46000\n",
      "46200\n",
      "46400\n",
      "46600\n",
      "46800\n",
      "47000\n",
      "47200\n",
      "47400\n",
      "47600\n",
      "47800\n",
      "48000\n",
      "48200\n",
      "48400\n",
      "48600\n",
      "48800\n",
      "49000\n",
      "49200\n",
      "49400\n",
      "49600\n",
      "49800\n",
      "50000\n",
      "50200\n",
      "50400\n",
      "50600\n",
      "50800\n",
      "51000\n",
      "51200\n",
      "51400\n",
      "51600\n",
      "51800\n",
      "52000\n",
      "52200\n",
      "52400\n",
      "52600\n",
      "52800\n",
      "53000\n",
      "53200\n",
      "53400\n",
      "53600\n",
      "53800\n",
      "54000\n",
      "54200\n",
      "54400\n",
      "54600\n",
      "54800\n",
      "55000\n",
      "55200\n",
      "55400\n",
      "55600\n",
      "55800\n",
      "56000\n",
      "56200\n",
      "56400\n",
      "56600\n",
      "56800\n",
      "57000\n",
      "57200\n",
      "57400\n",
      "57600\n",
      "57800\n",
      "58000\n",
      "58200\n",
      "58400\n",
      "58600\n",
      "58800\n",
      "59000\n",
      "59200\n",
      "59400\n",
      "59600\n",
      "59800\n",
      "60000\n",
      "60200\n",
      "60400\n",
      "60600\n",
      "60800\n",
      "61000\n",
      "61200\n",
      "61400\n",
      "61600\n",
      "61800\n",
      "62000\n",
      "62200\n",
      "62400\n",
      "62600\n",
      "62800\n",
      "63000\n",
      "63200\n",
      "63400\n",
      "63600\n",
      "63800\n",
      "64000\n",
      "64200\n",
      "64400\n",
      "64600\n",
      "64800\n",
      "65000\n",
      "65200\n",
      "65400\n",
      "65600\n",
      "65800\n",
      "66000\n",
      "66200\n",
      "66400\n",
      "66600\n",
      "66800\n",
      "67000\n",
      "67200\n",
      "67400\n",
      "67600\n",
      "67800\n",
      "68000\n",
      "68200\n",
      "68400\n",
      "68600\n",
      "68800\n",
      "69000\n",
      "69200\n",
      "69400\n",
      "69600\n",
      "69800\n",
      "70000\n",
      "70200\n",
      "70400\n",
      "70600\n",
      "70800\n",
      "71000\n",
      "71200\n",
      "71400\n",
      "71600\n",
      "71800\n",
      "72000\n",
      "72200\n",
      "72400\n",
      "72600\n",
      "72800\n",
      "73000\n",
      "73200\n",
      "73400\n",
      "73600\n",
      "73800\n",
      "74000\n",
      "74200\n",
      "74400\n",
      "74600\n",
      "74800\n",
      "75000\n",
      "75200\n",
      "75400\n",
      "75600\n",
      "75800\n",
      "76000\n",
      "76200\n",
      "76400\n",
      "76600\n",
      "76800\n",
      "77000\n",
      "77200\n",
      "77400\n",
      "77600\n",
      "77800\n",
      "78000\n",
      "78200\n",
      "78400\n",
      "78600\n",
      "78800\n",
      "79000\n",
      "79200\n",
      "79400\n",
      "79600\n",
      "79800\n",
      "80000\n",
      "80200\n",
      "80400\n",
      "80600\n",
      "80800\n",
      "81000\n",
      "81200\n",
      "81400\n",
      "81600\n",
      "81800\n",
      "82000\n",
      "82200\n",
      "82400\n",
      "82600\n",
      "82800\n",
      "83000\n",
      "83200\n",
      "83400\n",
      "83600\n",
      "83800\n",
      "84000\n",
      "84200\n",
      "84400\n",
      "84600\n",
      "84800\n",
      "85000\n",
      "85200\n",
      "85400\n",
      "85600\n",
      "85800\n",
      "86000\n",
      "86200\n",
      "86400\n",
      "86600\n",
      "86800\n",
      "87000\n",
      "87200\n",
      "87400\n",
      "87600\n",
      "87800\n",
      "88000\n",
      "88200\n",
      "88400\n",
      "88600\n",
      "88800\n",
      "89000\n",
      "89200\n",
      "89400\n",
      "89600\n",
      "89800\n",
      "90000\n",
      "90200\n",
      "90400\n",
      "90600\n",
      "90800\n",
      "91000\n",
      "91200\n",
      "91400\n",
      "91600\n",
      "91800\n",
      "92000\n",
      "92200\n",
      "92400\n",
      "92600\n",
      "92800\n",
      "93000\n",
      "93200\n",
      "93400\n",
      "93600\n",
      "93800\n",
      "94000\n",
      "94200\n",
      "94400\n",
      "94600\n",
      "94800\n",
      "95000\n",
      "95200\n",
      "95400\n",
      "95600\n",
      "95800\n",
      "96000\n",
      "96200\n",
      "96400\n",
      "96600\n",
      "96800\n",
      "97000\n",
      "97200\n",
      "97400\n",
      "97600\n",
      "97800\n",
      "98000\n",
      "98200\n",
      "98400\n",
      "98600\n",
      "98800\n",
      "99000\n",
      "99200\n",
      "99400\n",
      "99600\n",
      "99800\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,1\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  o         \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "  o       x \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,0\n",
      "-------------\n",
      "  x         \n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "  o       x \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  # train the agent\n",
    "  p1 = Agent2()\n",
    "  p2 = Agent4()\n",
    "\n",
    "  # set initial V for p1 and p2\n",
    "  env = Environment()\n",
    "  state_winner_triples = get_state_hash_and_winner(env)\n",
    "\n",
    "\n",
    "  Vx = initialV_x(env, state_winner_triples)\n",
    "  p1.setV(Vx)\n",
    "  Vo = initialV_o(env, state_winner_triples)\n",
    "  p2.setV(Vo)\n",
    "\n",
    "  # give each player their symbol\n",
    "  p1.set_symbol(env.x)\n",
    "  p2.set_symbol(env.o)\n",
    "\n",
    "  T = 100000\n",
    "  for t in range(T):\n",
    "    if t % 200 == 0:\n",
    "      print(t)\n",
    "    play_game(p1, p2, Environment())\n",
    "\n",
    "  # play human vs. agent\n",
    "  # do you think the agent learned to play the game well?\n",
    "  human = Human()\n",
    "  human.set_symbol(env.x)\n",
    "  while True:\n",
    "    p1.set_verbose(True)\n",
    "    play_game(human, p2, Environment(), draw=1)\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion :__\n",
    "\n",
    "Despite a plenty of tinkering, the AI is still unable to deal with going second. In short, this exercise highlights the interaction and learning of an agent within trained to operate within an environment, highlighting the power and possibilites of reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
